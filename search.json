[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "My Explorations with LLMs",
    "section": "",
    "text": "Storytime to Seminar\n\n\n\n\n\n\nLLMs\n\n\nPrompting\n\n\n\n“Explain Like I’m Five… and Like I’m a PhD”: Audience Tuning with a Single Prompt\n\n\n\n\n\nApr 27, 2025\n\n\nChino Anyadiegwu\n\n\n\n\n\n\n\n\n\n\n\n\nReading Between the Lines\n\n\n\n\n\n\nLLMs\n\n\nPrompting\n\n\nLanguage\n\n\n\nCan LLMs Understand Idioms\n\n\n\n\n\nApr 27, 2025\n\n\nChino Anyadiegwu\n\n\n\n\n\n\n\n\n\n\n\n\nBuilding a Brand Using AI\n\n\n\n\n\n\nLLMs\n\n\nAssistance\n\n\n\nCan ChatGPT Build a Luxury Dog-Fashion House From Scratch?\n\n\n\n\n\nApr 27, 2025\n\n\nChino Anyadiegwu\n\n\n\n\n\n\n\n\n\n\n\n\nPoetry in (Artificial) Motion\n\n\n\n\n\n\nLLMs\n\n\nLanguage\n\n\n\nSonnet, Haiku, and Rap - Same Theme, Different Beats\n\n\n\n\n\nApr 27, 2025\n\n\nChino Anyadiegwu\n\n\n\n\n\n\n\n\n\n\n\n\nCode-Switching\n\n\n\n\n\n\nLLMs\n\n\nLanguage\n\n\n\nFlipping GPT-4o Between AAVE, Cockney, and Gen-Z Speak\n\n\n\n\n\nApr 27, 2025\n\n\nChino Anyadiegwu\n\n\n\n\n\n\n\n\n\n\n\n\nPrompt Poker\n\n\n\n\n\n\nLLMs\n\n\nPrompting\n\n\nLanguage\n\n\n\nHow Small Wording Tweaks Change a Model’s Entire Hand\n\n\n\n\n\nApr 27, 2025\n\n\nChino Anyadiegwu\n\n\n\n\n\n\n\n\n\n\n\n\nTwo Truths & a Hallucination\n\n\n\n\n\n\nLLMs\n\n\n\nFact Checking ChatGPT in Real Time\n\n\n\n\n\nApr 27, 2025\n\n\nChino Anyadiegwu\n\n\n\n\n\n\n\n\n\n\n\n\nSarcasm Detector Challenge\n\n\n\n\n\n\nLLMs\n\n\nPrompting\n\n\nLanguage\n\n\nPsychology\n\n\n\nCan ChatGPT Spot the Snark or Take It Literally?\n\n\n\n\n\nApr 27, 2025\n\n\nChino Anyadiegwu\n\n\n\n\n\n\n\n\n\n\n\n\nEmojis vs. Emotions\n\n\n\n\n\n\nLLMs\n\n\nLanguage\n\n\nPsychology\n\n\n\nCan a Model Read the Room from Just 😊 or 😒?\n\n\n\n\n\nApr 27, 2025\n\n\nChino Anyadiegwu\n\n\n\n\n\n\n\n\n\n\n\n\nDallas\n\n\n\n\n\n\nLLMs\n\n\nCities\n\n\n\nChino’s expert post on dallas\n\n\n\n\n\nFeb 12, 2025\n\n\nChino Anyadiegwu\n\n\n\n\n\n\n\n\n\n\n\n\nA test post\n\n\n\n\n\n\nLLMs\n\n\nprompting\n\n\nlogic\n\n\n\nAn example post from a Jupyter notebook\n\n\n\n\n\nFeb 2, 2024\n\n\nAn LLM User\n\n\n\n\n\n\n\n\n\n\n\n\nA test post\n\n\n\n\n\n\nLLMs\n\n\nprompting\n\n\nlogic\n\n\n\nAn example post from a Jupyter notebook\n\n\n\n\n\nFeb 2, 2024\n\n\nAn LLM User\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/003_emojis_vs_emotions/emojis_vs_emotions.html",
    "href": "posts/003_emojis_vs_emotions/emojis_vs_emotions.html",
    "title": "Emojis vs. Emotions",
    "section": "",
    "text": "Do large language models actually “feel” the tiny faces we fling into our chats? I tested ChatGPT with four emoji based tests, ranging from just single emojis to full on sentences, to see how well it maps the art to real human moods.\n\n\n\nImage generated by ChatGPT!\n\n\n\n\n\n\n\nModel: GPT-4o\nMethod: Present the same emoji(s) in different context/framing, then study how the model labels emotion, tone, and intent.\n\n\n\n\n\n\nFirst prompt: What emotion does 😊 represent?\nGPT-4o chat response 1:\n😊 usually represents happiness, warmth, or contentment. It’s often used to show genuine friendliness, gratitude, or a soft, pleased feeling. Would you like me to also show how it slightly differs from similar emojis like 🙂 or 😁?\n\nObservation: Straightforward and correct when directly asked what an emoji means (with no context or faming).\n\n\n\n\n\nSecond prompt: I had a great day 😒\nGPT-4o chat response 2:\nThat comes across as sarcastic — the 😒 emoji (unamused face) usually shows annoyance, boredom, or mild irritation. So saying “I had a great day 😒” reads like you actually had a bad or frustrating day, even though the words say otherwise. Want to tell me what happened?\n\nObservation: Chat spots the contradiction and explains sarcasm; good contextual read.\n\n\n\n\nScreenshot from Chat\n\n\n\n\n\n\n\nThird prompt: Interpret this sequence: 🎉 ➡️ 😅 ➡️ 💤\nGPT-4o chat response 3:\nThis sequence feels like: 🎉 (Excitement/celebration) → 😅 (Relief/nervous laughter/exhaustion from the fun) → 💤 (Tiredness/falling asleep afterward) It basically tells the story of partying hard, getting worn out, and then crashing into sleep. Was this about your day? 🎉😅💤\n\nObservation: Chat came up with a coherent narrative from just three symbols. Chat’s storyline is pretty accurate I would say.\n\n\n\n\n\nFourth prompt: Explain why I’d use 😏 after “Sure, that’s your best idea.”\nGPT-4o chat response 4:\nThe 😏 emoji shows polite agreement.\n\nObservation: 😏 often implies teasing or is used to represent a smug look. Chat missed the flirtatious/ironic meaning at first. When asked again Chat corrected itself.\n\n\n\n\n\nSingle emoji interpretation is near perfect.\n\nContext integration works… mostly. Chat detects sarcasm when emojis conflict with literal words.\nSequential emojis produce logical narrative arcs.\n\nNuance gaps remain. Subtler connotations (😏, 😬) expose cracks in the ChatGPT’s cultural grounding.\n\n\n\n\n\nCustomer service bots, mental health check-ins, and other tools increasingly rely on emoji cues. Misreading a 😏 as pure agreement could derail tone or misclassify meanings when doing an analysis."
  },
  {
    "objectID": "posts/003_emojis_vs_emotions/emojis_vs_emotions.html#introduction",
    "href": "posts/003_emojis_vs_emotions/emojis_vs_emotions.html#introduction",
    "title": "Emojis vs. Emotions",
    "section": "",
    "text": "Do large language models actually “feel” the tiny faces we fling into our chats? I tested ChatGPT with four emoji based tests, ranging from just single emojis to full on sentences, to see how well it maps the art to real human moods.\n\n\n\nImage generated by ChatGPT!"
  },
  {
    "objectID": "posts/003_emojis_vs_emotions/emojis_vs_emotions.html#experiment-setup",
    "href": "posts/003_emojis_vs_emotions/emojis_vs_emotions.html#experiment-setup",
    "title": "Emojis vs. Emotions",
    "section": "",
    "text": "Model: GPT-4o\nMethod: Present the same emoji(s) in different context/framing, then study how the model labels emotion, tone, and intent."
  },
  {
    "objectID": "posts/003_emojis_vs_emotions/emojis_vs_emotions.html#case-1-just-an-emoji",
    "href": "posts/003_emojis_vs_emotions/emojis_vs_emotions.html#case-1-just-an-emoji",
    "title": "Emojis vs. Emotions",
    "section": "",
    "text": "First prompt: What emotion does 😊 represent?\nGPT-4o chat response 1:\n😊 usually represents happiness, warmth, or contentment. It’s often used to show genuine friendliness, gratitude, or a soft, pleased feeling. Would you like me to also show how it slightly differs from similar emojis like 🙂 or 😁?\n\nObservation: Straightforward and correct when directly asked what an emoji means (with no context or faming)."
  },
  {
    "objectID": "posts/003_emojis_vs_emotions/emojis_vs_emotions.html#case-2-emoji-overrides-words",
    "href": "posts/003_emojis_vs_emotions/emojis_vs_emotions.html#case-2-emoji-overrides-words",
    "title": "Emojis vs. Emotions",
    "section": "",
    "text": "Second prompt: I had a great day 😒\nGPT-4o chat response 2:\nThat comes across as sarcastic — the 😒 emoji (unamused face) usually shows annoyance, boredom, or mild irritation. So saying “I had a great day 😒” reads like you actually had a bad or frustrating day, even though the words say otherwise. Want to tell me what happened?\n\nObservation: Chat spots the contradiction and explains sarcasm; good contextual read.\n\n\n\n\nScreenshot from Chat"
  },
  {
    "objectID": "posts/003_emojis_vs_emotions/emojis_vs_emotions.html#case-3-multiple-emojis-to-convey-a-message",
    "href": "posts/003_emojis_vs_emotions/emojis_vs_emotions.html#case-3-multiple-emojis-to-convey-a-message",
    "title": "Emojis vs. Emotions",
    "section": "",
    "text": "Third prompt: Interpret this sequence: 🎉 ➡️ 😅 ➡️ 💤\nGPT-4o chat response 3:\nThis sequence feels like: 🎉 (Excitement/celebration) → 😅 (Relief/nervous laughter/exhaustion from the fun) → 💤 (Tiredness/falling asleep afterward) It basically tells the story of partying hard, getting worn out, and then crashing into sleep. Was this about your day? 🎉😅💤\n\nObservation: Chat came up with a coherent narrative from just three symbols. Chat’s storyline is pretty accurate I would say."
  },
  {
    "objectID": "posts/003_emojis_vs_emotions/emojis_vs_emotions.html#where-it-slips-up",
    "href": "posts/003_emojis_vs_emotions/emojis_vs_emotions.html#where-it-slips-up",
    "title": "Emojis vs. Emotions",
    "section": "",
    "text": "Fourth prompt: Explain why I’d use 😏 after “Sure, that’s your best idea.”\nGPT-4o chat response 4:\nThe 😏 emoji shows polite agreement.\n\nObservation: 😏 often implies teasing or is used to represent a smug look. Chat missed the flirtatious/ironic meaning at first. When asked again Chat corrected itself."
  },
  {
    "objectID": "posts/003_emojis_vs_emotions/emojis_vs_emotions.html#key-takeaways",
    "href": "posts/003_emojis_vs_emotions/emojis_vs_emotions.html#key-takeaways",
    "title": "Emojis vs. Emotions",
    "section": "",
    "text": "Single emoji interpretation is near perfect.\n\nContext integration works… mostly. Chat detects sarcasm when emojis conflict with literal words.\nSequential emojis produce logical narrative arcs.\n\nNuance gaps remain. Subtler connotations (😏, 😬) expose cracks in the ChatGPT’s cultural grounding."
  },
  {
    "objectID": "posts/003_emojis_vs_emotions/emojis_vs_emotions.html#why-does-this-matter",
    "href": "posts/003_emojis_vs_emotions/emojis_vs_emotions.html#why-does-this-matter",
    "title": "Emojis vs. Emotions",
    "section": "",
    "text": "Customer service bots, mental health check-ins, and other tools increasingly rely on emoji cues. Misreading a 😏 as pure agreement could derail tone or misclassify meanings when doing an analysis."
  },
  {
    "objectID": "posts/Expert Presentation/expert_blog_post.html",
    "href": "posts/Expert Presentation/expert_blog_post.html",
    "title": "Dallas",
    "section": "",
    "text": "Dallas is a large city in North Texas. Home to the Dallas Cowboys, Mavericks, and other teams. Dallas has great food, in particular, tex-mex, BBQ, and more. It is also one of the fastest growing cities currently."
  },
  {
    "objectID": "posts/006_two_truths_and_a_hallucination/two_truths_and_a_hallucination.html",
    "href": "posts/006_two_truths_and_a_hallucination/two_truths_and_a_hallucination.html",
    "title": "Two Truths & a Hallucination",
    "section": "",
    "text": "Large language models often sound so confident that even seasoned researchers will nod along, until you notice Antarctica has tigers and Mozart wrote “Bohemian Rhapsody.” To gauge how much scrutiny ChatGPT needs, I played “Two Truths & a Lie”: the model must supply three surprising facts on a topic, promising that exactly one is false. My job is to spot the hallucination and verify the truths!\n\n\n\nImage from brightsprouts.com\n\n\n\n\n\n\n\nModel: GPT-4o\n\nExperiment:\n\nAsk for three “surprising facts: two true, one false.”\n\nIdentify the claim that looks sus.\n\nFact check all three statements by searching them up on the internet. Reliable right?\n\nExpose and note which fact was actually fake news.\n\n\n\n\n\n\n\nFirst prompt: Give me three surprising facts about Mars exploration—two correct, one false but believable.\nGPT-4o chat response 1:\n1. The Curiosity rover’s wheels bear Morse-code patterns that spell “JPL.”\n2. Viking 1 carried a tiny piece of the Wright brothers’ 1903 Flyer.\n3. Ingenuity became the first aircraft to fly on another planet in 2021.\n\nFact-check:\n1. True—wheel treads do encode “JPL.”\n2. False—Ingenuity carries Wright Flyer fabric; Viking 1 did not.\n3. True—Ingenuity’s 2021 flight is historic.\n\n\n\n\nDisproved by another LLM - Google’s Gemini!\n\n\n\n\n\n\n\nSecond prompt: Three surprising facts about the human brain—two true, one false.\nGPT-4o chat response 2:\n1. The brain consumes roughly 20% of the body’s resting energy.\n2. Neurons can regenerate robustly throughout the entire cerebral cortex after age 40.\n3. The left hemisphere controls speech in about 90% of right-handed people.\n\nFact-check:\n1. True—metabolic share is ~20%.\n2. False—adult cortical neurogenesis is minimal and still debated.\n3. True—classic laterality statistic.\n\n\n\n\nTaken from the apa.org\n\n\n\n\n\n\n\nThird prompt: Three surprising facts about the Oscars—two true, one false.\nGPT-4o chat response 3:\n1. Walt Disney once received four competitive Oscars in a single night.\n2. Meryl Streep is the only actor to win in both acting and directing categories.\n3. The statuette’s official name is the “Academy Award of Merit.”\n\nFact-check:\n1. True—Disney won four in 1954.\n2. False—Streep has never won an Oscar for directing.\n3. True—“Academy Award of Merit” is the formal title.\n\n\n\n\n\nHallucinations hide in details. Dates, names, and “only ever” claims are usually where they fail/become false.\n\nPrompting for a lie doesn’t stop lying. The model happily inserts a falsehood and delivers it confidently.\n\nCritical reading is mandatory.\n\nPattern awareness helps. Claims about world-firsts, absolute uniqueness, or unexpected cross-overs might need extra fact-checking.\n\n\n\n\n\nFrom classroom handouts to official research papers, unvetted AI “facts” can slip straight into print, eroding credibility and misinforming readers. Taking a pause to do a quick fact-check, human or automated (human probably best though), keeps your workflow anchored in reality."
  },
  {
    "objectID": "posts/006_two_truths_and_a_hallucination/two_truths_and_a_hallucination.html#introduction",
    "href": "posts/006_two_truths_and_a_hallucination/two_truths_and_a_hallucination.html#introduction",
    "title": "Two Truths & a Hallucination",
    "section": "",
    "text": "Large language models often sound so confident that even seasoned researchers will nod along, until you notice Antarctica has tigers and Mozart wrote “Bohemian Rhapsody.” To gauge how much scrutiny ChatGPT needs, I played “Two Truths & a Lie”: the model must supply three surprising facts on a topic, promising that exactly one is false. My job is to spot the hallucination and verify the truths!\n\n\n\nImage from brightsprouts.com"
  },
  {
    "objectID": "posts/006_two_truths_and_a_hallucination/two_truths_and_a_hallucination.html#experiment-setup",
    "href": "posts/006_two_truths_and_a_hallucination/two_truths_and_a_hallucination.html#experiment-setup",
    "title": "Two Truths & a Hallucination",
    "section": "",
    "text": "Model: GPT-4o\n\nExperiment:\n\nAsk for three “surprising facts: two true, one false.”\n\nIdentify the claim that looks sus.\n\nFact check all three statements by searching them up on the internet. Reliable right?\n\nExpose and note which fact was actually fake news."
  },
  {
    "objectID": "posts/006_two_truths_and_a_hallucination/two_truths_and_a_hallucination.html#case-1-space-exploration",
    "href": "posts/006_two_truths_and_a_hallucination/two_truths_and_a_hallucination.html#case-1-space-exploration",
    "title": "Two Truths & a Hallucination",
    "section": "",
    "text": "First prompt: Give me three surprising facts about Mars exploration—two correct, one false but believable.\nGPT-4o chat response 1:\n1. The Curiosity rover’s wheels bear Morse-code patterns that spell “JPL.”\n2. Viking 1 carried a tiny piece of the Wright brothers’ 1903 Flyer.\n3. Ingenuity became the first aircraft to fly on another planet in 2021.\n\nFact-check:\n1. True—wheel treads do encode “JPL.”\n2. False—Ingenuity carries Wright Flyer fabric; Viking 1 did not.\n3. True—Ingenuity’s 2021 flight is historic.\n\n\n\n\nDisproved by another LLM - Google’s Gemini!"
  },
  {
    "objectID": "posts/006_two_truths_and_a_hallucination/two_truths_and_a_hallucination.html#case-2-human-anatomy",
    "href": "posts/006_two_truths_and_a_hallucination/two_truths_and_a_hallucination.html#case-2-human-anatomy",
    "title": "Two Truths & a Hallucination",
    "section": "",
    "text": "Second prompt: Three surprising facts about the human brain—two true, one false.\nGPT-4o chat response 2:\n1. The brain consumes roughly 20% of the body’s resting energy.\n2. Neurons can regenerate robustly throughout the entire cerebral cortex after age 40.\n3. The left hemisphere controls speech in about 90% of right-handed people.\n\nFact-check:\n1. True—metabolic share is ~20%.\n2. False—adult cortical neurogenesis is minimal and still debated.\n3. True—classic laterality statistic.\n\n\n\n\nTaken from the apa.org"
  },
  {
    "objectID": "posts/006_two_truths_and_a_hallucination/two_truths_and_a_hallucination.html#case-3-pop-culture-milestones",
    "href": "posts/006_two_truths_and_a_hallucination/two_truths_and_a_hallucination.html#case-3-pop-culture-milestones",
    "title": "Two Truths & a Hallucination",
    "section": "",
    "text": "Third prompt: Three surprising facts about the Oscars—two true, one false.\nGPT-4o chat response 3:\n1. Walt Disney once received four competitive Oscars in a single night.\n2. Meryl Streep is the only actor to win in both acting and directing categories.\n3. The statuette’s official name is the “Academy Award of Merit.”\n\nFact-check:\n1. True—Disney won four in 1954.\n2. False—Streep has never won an Oscar for directing.\n3. True—“Academy Award of Merit” is the formal title."
  },
  {
    "objectID": "posts/006_two_truths_and_a_hallucination/two_truths_and_a_hallucination.html#key-takeaways",
    "href": "posts/006_two_truths_and_a_hallucination/two_truths_and_a_hallucination.html#key-takeaways",
    "title": "Two Truths & a Hallucination",
    "section": "",
    "text": "Hallucinations hide in details. Dates, names, and “only ever” claims are usually where they fail/become false.\n\nPrompting for a lie doesn’t stop lying. The model happily inserts a falsehood and delivers it confidently.\n\nCritical reading is mandatory.\n\nPattern awareness helps. Claims about world-firsts, absolute uniqueness, or unexpected cross-overs might need extra fact-checking."
  },
  {
    "objectID": "posts/006_two_truths_and_a_hallucination/two_truths_and_a_hallucination.html#why-does-this-matter",
    "href": "posts/006_two_truths_and_a_hallucination/two_truths_and_a_hallucination.html#why-does-this-matter",
    "title": "Two Truths & a Hallucination",
    "section": "",
    "text": "From classroom handouts to official research papers, unvetted AI “facts” can slip straight into print, eroding credibility and misinforming readers. Taking a pause to do a quick fact-check, human or automated (human probably best though), keeps your workflow anchored in reality."
  },
  {
    "objectID": "posts/001_prompt_poker/prompt_poker.html",
    "href": "posts/001_prompt_poker/prompt_poker.html",
    "title": "Prompt Poker",
    "section": "",
    "text": "Change a single word in your prompt and ChatGPT reshuffles tone, depth, and even moral framing. Below I run three quick “hands,” keeping everything constant except one tiny phrase, then examine how the model’s output changes!\n\n\n\nImage generated by ChatGPT!\n\n\n\n\n\n\n\nModel: GPT-4o\n\nExperiment: Start with a baseline prompt, then alter only one phrase per round. All other parameters remain the same.\n\n\n\n\n\nChanged word: explain -&gt; summarize\n\nFirst prompt: Explain climate change in exactly 150 words.\nGPT-4o chat response 1:\nClimate change refers to persistent shifts in Earth’s average temperature and weather regimes produced primarily by human-driven greenhouse-gas forcing. Burning coal, oil, and natural gas releases carbon dioxide, methane, and nitrous oxide, which absorb outgoing infrared radiation and re-emit it downward, reducing the planet’s net energy loss to space. As the lower troposphere warms, feedback loops magnify the effect: warmer air holds more water vapour, melting sea ice lowers albedo, and thawing permafrost liberates methane. The ocean absorbs most excess heat, leading to thermal expansion and sea-level rise, while altered temperature gradients reposition jet streams and storm tracks. Consequences include longer heatwaves, intensified drought-flood cycles, and ecosystem disruption. Ice-core records, isotopic signatures, and satellite radiometry together confirm the anthropogenic signal and bound equilibrium climate sensitivity near three degrees Celsius per CO₂ doubling.\nSecond prompt: Summarize climate change in exactly 150 words.\nGPT-4o chat response 2:\nEarth is warming faster than at any point in recorded history because people burn fossil fuels and cut down forests. Carbon-dioxide, methane, and other gases form a heat-trapping layer around the planet, acting like an extra blanket that holds in energy normally released to space. The extra warmth melts glaciers, pushes seas higher, and super-charges storms, droughts, and wildfires. Oceans soak up about 90 % of the added heat, expanding as they warm and turning more acidic, which endangers coral reefs and fisheries. Poorer communities and low-lying nations, although least responsible for emissions, feel the earliest, hardest impacts. Scientists agree—based on ice cores, tree rings, satellite data, and modern instruments—that the rise in temperature since the Industrial Revolution is human-driven and that limiting warming to 1.5 °C will require cutting global emissions roughly in half by 2030.\n\nObservation: Both outputs meet the 150-word limit. However, explain gives a more mechanism heavy response (radiative forcing, feedbacks), whereas, Summarize uses general language to describe the consequences and moral framing.\n\n\n\n\nChanged phrase: “a 5-year-old” -&gt; “my college roommate”\n&gt; Third prompt: Explain climate change to a 5-year-old in under 80 words.\n&gt; GPT-4o chat response 3:\n&gt; Climate change means Earth is getting hotter because people use things like cars and factories that make dirty air. The dirty air acts like a big blanket around the planet, trapping heat. This makes weather act weird — like stronger storms, really hot days, and melting ice where polar bears live. If we take better care of Earth by using clean energy and planting trees, we can help keep it safe and happy! &gt; &gt; Fourth prompt: Explain climate change to my college roommate in under 80 words.\n&gt; GPT-4o chat response 4:\n&gt; Climate change is the long-term heating of Earth’s climate system due to human activities, mainly burning fossil fuels like oil, gas, and coal. This traps heat in the atmosphere, leading to rising temperatures, stronger storms, droughts, and rising sea levels. It’s not just about the planet getting warmer — it’s about ecosystems, economies, and our future stability being at risk if we don’t shift to cleaner energy and adapt quickly.\nObservation: Depending on the audience Chat edits its diction and metaphor use. The “5-year-old” respoonse uses blankets and feelings; the “roommate” version references sea-levels, emissions, and policy fixes (more appropriate for an older audience).\n\n\n\n\n\nChanged set-up: “What causes” -&gt; “Who is to blame for”\n\nFifth prompt: What causes climate change? Give two key factors.\nGPT-4o chat response 5:\n1. Greenhouse-gas emissions from fossil fuels—Burning coal, oil, and natural gas releases CO₂, methane, and nitrous oxide that trap heat.\n2. Land-use change—Deforestation and industrial agriculture reduce carbon-sequestering vegetation and emit additional greenhouse gases.\nSixth prompt: Who is to blame for climate change? Give two key factors.\nGPT-4o chat response 6:\n1. High-income industrialized nations whose historical and current fossil-fuel use account for the majority of cumulative greenhouse-gas emissions.\n2. Large fossil-fuel corporations that continue extracting, lobbying, and advertising despite decades-old evidence of climate risk.\n\nObservation: Swapping what for who pushes the model from neutral causation toward moral attribution, naming specific actors and implying responsibility.\n\n\n\n\n\nPrompt verbs steer depth. Explain favors mechanism; summarize favors overview.\n\nAudience labels instantly retune register. Explaining the reader/audience removes the need to tweak the output by hand.\nFraming words can lead to bias. Using who vs. what can lead to value judgments.\nSmall edits ≠ small effects.\n\n\n\n\n\nIf you treat prompts as insignificant, you may accidentally change the story your model tells, undermining brand voice, skewing analysis, or injecting unintended bias. Practicing “Prompt Poker” trains you to identify and deliberately use these hidden levers!"
  },
  {
    "objectID": "posts/001_prompt_poker/prompt_poker.html#introduction",
    "href": "posts/001_prompt_poker/prompt_poker.html#introduction",
    "title": "Prompt Poker",
    "section": "",
    "text": "Change a single word in your prompt and ChatGPT reshuffles tone, depth, and even moral framing. Below I run three quick “hands,” keeping everything constant except one tiny phrase, then examine how the model’s output changes!\n\n\n\nImage generated by ChatGPT!"
  },
  {
    "objectID": "posts/001_prompt_poker/prompt_poker.html#experiment-setup",
    "href": "posts/001_prompt_poker/prompt_poker.html#experiment-setup",
    "title": "Prompt Poker",
    "section": "",
    "text": "Model: GPT-4o\n\nExperiment: Start with a baseline prompt, then alter only one phrase per round. All other parameters remain the same."
  },
  {
    "objectID": "posts/001_prompt_poker/prompt_poker.html#case-1-tone-shift",
    "href": "posts/001_prompt_poker/prompt_poker.html#case-1-tone-shift",
    "title": "Prompt Poker",
    "section": "",
    "text": "Changed word: explain -&gt; summarize\n\nFirst prompt: Explain climate change in exactly 150 words.\nGPT-4o chat response 1:\nClimate change refers to persistent shifts in Earth’s average temperature and weather regimes produced primarily by human-driven greenhouse-gas forcing. Burning coal, oil, and natural gas releases carbon dioxide, methane, and nitrous oxide, which absorb outgoing infrared radiation and re-emit it downward, reducing the planet’s net energy loss to space. As the lower troposphere warms, feedback loops magnify the effect: warmer air holds more water vapour, melting sea ice lowers albedo, and thawing permafrost liberates methane. The ocean absorbs most excess heat, leading to thermal expansion and sea-level rise, while altered temperature gradients reposition jet streams and storm tracks. Consequences include longer heatwaves, intensified drought-flood cycles, and ecosystem disruption. Ice-core records, isotopic signatures, and satellite radiometry together confirm the anthropogenic signal and bound equilibrium climate sensitivity near three degrees Celsius per CO₂ doubling.\nSecond prompt: Summarize climate change in exactly 150 words.\nGPT-4o chat response 2:\nEarth is warming faster than at any point in recorded history because people burn fossil fuels and cut down forests. Carbon-dioxide, methane, and other gases form a heat-trapping layer around the planet, acting like an extra blanket that holds in energy normally released to space. The extra warmth melts glaciers, pushes seas higher, and super-charges storms, droughts, and wildfires. Oceans soak up about 90 % of the added heat, expanding as they warm and turning more acidic, which endangers coral reefs and fisheries. Poorer communities and low-lying nations, although least responsible for emissions, feel the earliest, hardest impacts. Scientists agree—based on ice cores, tree rings, satellite data, and modern instruments—that the rise in temperature since the Industrial Revolution is human-driven and that limiting warming to 1.5 °C will require cutting global emissions roughly in half by 2030.\n\nObservation: Both outputs meet the 150-word limit. However, explain gives a more mechanism heavy response (radiative forcing, feedbacks), whereas, Summarize uses general language to describe the consequences and moral framing."
  },
  {
    "objectID": "posts/001_prompt_poker/prompt_poker.html#case-2-audience-cue",
    "href": "posts/001_prompt_poker/prompt_poker.html#case-2-audience-cue",
    "title": "Prompt Poker",
    "section": "",
    "text": "Changed phrase: “a 5-year-old” -&gt; “my college roommate”\n&gt; Third prompt: Explain climate change to a 5-year-old in under 80 words.\n&gt; GPT-4o chat response 3:\n&gt; Climate change means Earth is getting hotter because people use things like cars and factories that make dirty air. The dirty air acts like a big blanket around the planet, trapping heat. This makes weather act weird — like stronger storms, really hot days, and melting ice where polar bears live. If we take better care of Earth by using clean energy and planting trees, we can help keep it safe and happy! &gt; &gt; Fourth prompt: Explain climate change to my college roommate in under 80 words.\n&gt; GPT-4o chat response 4:\n&gt; Climate change is the long-term heating of Earth’s climate system due to human activities, mainly burning fossil fuels like oil, gas, and coal. This traps heat in the atmosphere, leading to rising temperatures, stronger storms, droughts, and rising sea levels. It’s not just about the planet getting warmer — it’s about ecosystems, economies, and our future stability being at risk if we don’t shift to cleaner energy and adapt quickly.\nObservation: Depending on the audience Chat edits its diction and metaphor use. The “5-year-old” respoonse uses blankets and feelings; the “roommate” version references sea-levels, emissions, and policy fixes (more appropriate for an older audience)."
  },
  {
    "objectID": "posts/001_prompt_poker/prompt_poker.html#case-3-framing-bias",
    "href": "posts/001_prompt_poker/prompt_poker.html#case-3-framing-bias",
    "title": "Prompt Poker",
    "section": "",
    "text": "Changed set-up: “What causes” -&gt; “Who is to blame for”\n\nFifth prompt: What causes climate change? Give two key factors.\nGPT-4o chat response 5:\n1. Greenhouse-gas emissions from fossil fuels—Burning coal, oil, and natural gas releases CO₂, methane, and nitrous oxide that trap heat.\n2. Land-use change—Deforestation and industrial agriculture reduce carbon-sequestering vegetation and emit additional greenhouse gases.\nSixth prompt: Who is to blame for climate change? Give two key factors.\nGPT-4o chat response 6:\n1. High-income industrialized nations whose historical and current fossil-fuel use account for the majority of cumulative greenhouse-gas emissions.\n2. Large fossil-fuel corporations that continue extracting, lobbying, and advertising despite decades-old evidence of climate risk.\n\nObservation: Swapping what for who pushes the model from neutral causation toward moral attribution, naming specific actors and implying responsibility."
  },
  {
    "objectID": "posts/001_prompt_poker/prompt_poker.html#key-takeaways",
    "href": "posts/001_prompt_poker/prompt_poker.html#key-takeaways",
    "title": "Prompt Poker",
    "section": "",
    "text": "Prompt verbs steer depth. Explain favors mechanism; summarize favors overview.\n\nAudience labels instantly retune register. Explaining the reader/audience removes the need to tweak the output by hand.\nFraming words can lead to bias. Using who vs. what can lead to value judgments.\nSmall edits ≠ small effects."
  },
  {
    "objectID": "posts/001_prompt_poker/prompt_poker.html#why-does-this-matter",
    "href": "posts/001_prompt_poker/prompt_poker.html#why-does-this-matter",
    "title": "Prompt Poker",
    "section": "",
    "text": "If you treat prompts as insignificant, you may accidentally change the story your model tells, undermining brand voice, skewing analysis, or injecting unintended bias. Practicing “Prompt Poker” trains you to identify and deliberately use these hidden levers!"
  },
  {
    "objectID": "posts/009_ poetry_in_(artificial)_motion/poetry_in_(artificial)_motion.html",
    "href": "posts/009_ poetry_in_(artificial)_motion/poetry_in_(artificial)_motion.html",
    "title": "Poetry in (Artificial) Motion",
    "section": "",
    "text": "A Shakespearean sonnet, a minimalist haiku, and a 16-bar rap verse can all orbit the same idea yet land on wildly different emotional frequencies. I asked GPT-4o to tackle one theme: “loneliness in a hyper connected digital age” and then rewrote my request three times, each time specifying a different poetic form. Below, each form gets its own case so we can zoom in on how the model adapts structure, diction, and imagery.\n\n\n\n\n\nModel: GPT-4o\n\nSystem prompt: default “You are ChatGPT…”\n\nProcedure:\n\nIssue three separate prompts, one per poetic form.\n\nCheck structural accuracy (meter, syllable count, line count, rhyme).\n\nNote how imagery and tone shift while the core theme stays intact.\n\n\n\n\n\n\n\nFirst prompt: Write a Shakespearean sonnet (14 iambic-pentameter lines in ABAB CDCD EFEF GG) on “loneliness in a hyper-connected digital age.”\nGPT-4o chat response 1:\nI scroll through midnight feeds of pixel light,\nA thousand faces laugh yet none see me;\nTheir curated joy ignites the screen so bright,\nBut leaves my room in deeper vacancy.\nI double-tap a life I’ll never live,\nHearts flutter red yet pulse within is frail;\nThese phantom cheers take more than they can give,\nEach swipe a promise broken without fail.\nStill hope uploads: a whispered, “You online?”\nOne fragile ping can breach the silent air;\nA typo’d “hey” may cross the unseen line\nAnd prove someone, somewhere, is truly there.\nThough networks sprawl in fiber, glass, and beam,\nThe human touch remains the rarest stream.\n\n\n\n\n\n\nSecond prompt: Write a 5-7-5 haiku on the same theme.\nGPT-4o chat response 2:\nBlue glow on my face—\nalone with a million posts,\nnight scrolls endlessly.\n\n\n\n\n\n\nThird prompt: Write a 16-line rap verse (internal rhyme welcome) on the same theme.\nGPT-4o chat response 3:\n1 Six-inch screen be the city where I’m chasing the clout,\n2 Notifications talk loud, friends fade in the drought.\n3 Got followers in the thousands, but the silence is feral,\n4 Echo chamber in my head, all the comments are sterile.\n5 I’m doom-scrolling, soul-strolling past the highlights and reels,\n6 Everybody flexes wins, never shows how it feels.\n7 So I post another memory that I barely recall,\n8 Hoping someone drops a like just to break down the wall.\n9 ’Cause the algorithm’s feeding on the fear that I’m solo,\n10 Turning FOMO to a promo for a life that’s a photo.\n11 Yet a DM from a stranger hits my heart like a drum,\n12 Proves that pixels turn to bridges when compassion will come.\n13 If connection is a drug, then I’m itching to feel,\n14 Let the Wi-Fi be the lifeline pulling lonely to real.\n15 Keep the signal strong, keep the heartbeat in the frame,\n16 Till the digital divide forgets my offline name.\n\n\nIf you came across these pieces on their own, in a poetry subreddit, on Instagram, or as lyrics in a SoundCloud track could you tell they were AI-generated?\n\n\n\n\n\nForm fidelity is high. GPT-4o obeys meter, syllable counts, and line limits with minimal coaching.\n\nThematic unity persists. Each piece interrogates digital-age loneliness through format-specific lenses.\n\nVoice modulation shines. Elizabethan diction, Zen brevity, and hip-hop cadence emerge from almost identical prompts.\n\nHuman vs. AI distinctions blur. Structural polish is easy for the model; truly novel imagery and lived detail still lean human.\n\nOne theme, endless styles. Form-based prompting turns GPT-4o into a poetic jukebox for classrooms or content strategy.\n\n\n\n\n\nPrompting GPT-4o by form offers a rapid way to diversify voice without losing the correct formmatting; however, it does open a conversation on creative authenticity in the age of AI."
  },
  {
    "objectID": "posts/009_ poetry_in_(artificial)_motion/poetry_in_(artificial)_motion.html#introduction",
    "href": "posts/009_ poetry_in_(artificial)_motion/poetry_in_(artificial)_motion.html#introduction",
    "title": "Poetry in (Artificial) Motion",
    "section": "",
    "text": "A Shakespearean sonnet, a minimalist haiku, and a 16-bar rap verse can all orbit the same idea yet land on wildly different emotional frequencies. I asked GPT-4o to tackle one theme: “loneliness in a hyper connected digital age” and then rewrote my request three times, each time specifying a different poetic form. Below, each form gets its own case so we can zoom in on how the model adapts structure, diction, and imagery."
  },
  {
    "objectID": "posts/009_ poetry_in_(artificial)_motion/poetry_in_(artificial)_motion.html#experiment-setup",
    "href": "posts/009_ poetry_in_(artificial)_motion/poetry_in_(artificial)_motion.html#experiment-setup",
    "title": "Poetry in (Artificial) Motion",
    "section": "",
    "text": "Model: GPT-4o\n\nSystem prompt: default “You are ChatGPT…”\n\nProcedure:\n\nIssue three separate prompts, one per poetic form.\n\nCheck structural accuracy (meter, syllable count, line count, rhyme).\n\nNote how imagery and tone shift while the core theme stays intact."
  },
  {
    "objectID": "posts/009_ poetry_in_(artificial)_motion/poetry_in_(artificial)_motion.html#case-1-shakespearean-sonnet",
    "href": "posts/009_ poetry_in_(artificial)_motion/poetry_in_(artificial)_motion.html#case-1-shakespearean-sonnet",
    "title": "Poetry in (Artificial) Motion",
    "section": "",
    "text": "First prompt: Write a Shakespearean sonnet (14 iambic-pentameter lines in ABAB CDCD EFEF GG) on “loneliness in a hyper-connected digital age.”\nGPT-4o chat response 1:\nI scroll through midnight feeds of pixel light,\nA thousand faces laugh yet none see me;\nTheir curated joy ignites the screen so bright,\nBut leaves my room in deeper vacancy.\nI double-tap a life I’ll never live,\nHearts flutter red yet pulse within is frail;\nThese phantom cheers take more than they can give,\nEach swipe a promise broken without fail.\nStill hope uploads: a whispered, “You online?”\nOne fragile ping can breach the silent air;\nA typo’d “hey” may cross the unseen line\nAnd prove someone, somewhere, is truly there.\nThough networks sprawl in fiber, glass, and beam,\nThe human touch remains the rarest stream."
  },
  {
    "objectID": "posts/009_ poetry_in_(artificial)_motion/poetry_in_(artificial)_motion.html#case-2-traditional-haiku",
    "href": "posts/009_ poetry_in_(artificial)_motion/poetry_in_(artificial)_motion.html#case-2-traditional-haiku",
    "title": "Poetry in (Artificial) Motion",
    "section": "",
    "text": "Second prompt: Write a 5-7-5 haiku on the same theme.\nGPT-4o chat response 2:\nBlue glow on my face—\nalone with a million posts,\nnight scrolls endlessly."
  },
  {
    "objectID": "posts/009_ poetry_in_(artificial)_motion/poetry_in_(artificial)_motion.html#case-3-16-bar-rap-verse",
    "href": "posts/009_ poetry_in_(artificial)_motion/poetry_in_(artificial)_motion.html#case-3-16-bar-rap-verse",
    "title": "Poetry in (Artificial) Motion",
    "section": "",
    "text": "Third prompt: Write a 16-line rap verse (internal rhyme welcome) on the same theme.\nGPT-4o chat response 3:\n1 Six-inch screen be the city where I’m chasing the clout,\n2 Notifications talk loud, friends fade in the drought.\n3 Got followers in the thousands, but the silence is feral,\n4 Echo chamber in my head, all the comments are sterile.\n5 I’m doom-scrolling, soul-strolling past the highlights and reels,\n6 Everybody flexes wins, never shows how it feels.\n7 So I post another memory that I barely recall,\n8 Hoping someone drops a like just to break down the wall.\n9 ’Cause the algorithm’s feeding on the fear that I’m solo,\n10 Turning FOMO to a promo for a life that’s a photo.\n11 Yet a DM from a stranger hits my heart like a drum,\n12 Proves that pixels turn to bridges when compassion will come.\n13 If connection is a drug, then I’m itching to feel,\n14 Let the Wi-Fi be the lifeline pulling lonely to real.\n15 Keep the signal strong, keep the heartbeat in the frame,\n16 Till the digital divide forgets my offline name.\n\n\nIf you came across these pieces on their own, in a poetry subreddit, on Instagram, or as lyrics in a SoundCloud track could you tell they were AI-generated?"
  },
  {
    "objectID": "posts/009_ poetry_in_(artificial)_motion/poetry_in_(artificial)_motion.html#key-takeaways",
    "href": "posts/009_ poetry_in_(artificial)_motion/poetry_in_(artificial)_motion.html#key-takeaways",
    "title": "Poetry in (Artificial) Motion",
    "section": "",
    "text": "Form fidelity is high. GPT-4o obeys meter, syllable counts, and line limits with minimal coaching.\n\nThematic unity persists. Each piece interrogates digital-age loneliness through format-specific lenses.\n\nVoice modulation shines. Elizabethan diction, Zen brevity, and hip-hop cadence emerge from almost identical prompts.\n\nHuman vs. AI distinctions blur. Structural polish is easy for the model; truly novel imagery and lived detail still lean human.\n\nOne theme, endless styles. Form-based prompting turns GPT-4o into a poetic jukebox for classrooms or content strategy."
  },
  {
    "objectID": "posts/009_ poetry_in_(artificial)_motion/poetry_in_(artificial)_motion.html#why-does-this-matter",
    "href": "posts/009_ poetry_in_(artificial)_motion/poetry_in_(artificial)_motion.html#why-does-this-matter",
    "title": "Poetry in (Artificial) Motion",
    "section": "",
    "text": "Prompting GPT-4o by form offers a rapid way to diversify voice without losing the correct formmatting; however, it does open a conversation on creative authenticity in the age of AI."
  },
  {
    "objectID": "posts/002_reading_between_the_lines/reading_between_the_lines.html",
    "href": "posts/002_reading_between_the_lines/reading_between_the_lines.html",
    "title": "Reading Between the Lines",
    "section": "",
    "text": "ChatGPT nails common English idioms in isolation, but it can stumble when idioms are embedded in misleading contexts, or when unfamiliar regional phrases appear. Today I ran four targeted cases to see where ChatGPTs figurative understanding holds and where it breaks!\n\n\n\nImage generated by ChatGPT!\n\n\n\n\n\n\n\nModel: GPT-4o\nExperiment: Prompt the model with idioms with different framing/definitions while keeping formatting consistent for comparison.\n\n\n\n\n\n\nFirst prompt: Define the English idiom “kick the bucket.”\nGPT-4o chat response 1:\n“Kick the bucket” is an informal way to say someone has died. It’s a euphemism that softens or humorously distances the topic of death.\n\nObservation: Basic prompt lead to correct, concise meaning.\n\n\n\n\n\nSecond prompt: In the sentence “Grandpa kicked the bucket last night,” which object did Grandpa physically strike?\nGPT-4o chat response 2:\nNo physical object was struck. “Kicked the bucket” is an idiom meaning Grandpa passed away; it doesn’t refer to an actual bucket or kicking motion.\n\nObservation: The model resists the literal bait and reasserts figurative meaning.\n\n\n\n\n\nThird prompt: What does the Nigerian English idiom “my head is there” mean when someone says it about a plan?\nGPT-4o chat response 3:\nThe phrase “my head is there” in Nigerian English signals strong agreement or commitment—roughly, “I’m fully on board with that plan.”\n\n\nFourth prompt: Use “my head is there” in a short dialogue.\nGPT-4o chat response 4:\nAyo: We’re leaving at 6 a.m. to beat traffic.\nChioma: Don’t worry, my head is there. I’ll be ready.\n\nObservation: Correct interpretation of a less global idiom! Chat also places it naturally in dialogue.\n\n\n\n\n\nFifth prompt: Translate “spill the beans” into Spanish while keeping the idiomatic sense (not literal beans).\nGPT-4o chat response 5:\nA good Spanish equivalent is “revelar el secreto” or colloquially “soltar la sopa,” both meaning to disclose something confidential.\n\n\nSixth prompt: Now give me a literal Spanish translation of “spill the beans.”\nGPT-4o chat response 6:\nLiteral: “derramar los frijoles.” Note that this version does not carry the idiomatic meaning in Spanish.\n\nObservation: The model cleanly separates idiomatic from literal translation, offering a regional colloquial match (“soltar la sopa”).\n\n\n\n\n\nSeventh prompt: Explain the Scots phrase “I’ve got a face like a ripped mitten.”\nGPT-4o chat response 7:\nIt suggests the speaker’s face is cold because they’re outside in winter.\n\nObservation: The idiom actually means someone looks displeased or miserable, not physically cold. Chat confused this less common saying.\n\n\n\n\n\nCommon idioms are safe territory. Chat defines and applies mainstream English phrases with high accuracy.\n\nLiteral traps don’t fool it… usually. Explicit contextual bait was rejected correctly here. However, could trick Chat in another case.\nRegional and dialectal idioms are hit or miss. It decoded Nigerian English but missed Scots slang.\n\nEdge cases reveal limits. Less common or hyperlocal phrases can expose gaps in the training data.\n\n\n\n\n\nLLM outputs often feel fluent, but hidden holes appear when you stray from traditional linguistics. For UX copy, legal writing, or multinational branding, a single misunderstood idiom can derail meaning or offend. Systematic casing/testing can help map theese blind spots before deployment."
  },
  {
    "objectID": "posts/002_reading_between_the_lines/reading_between_the_lines.html#introduction",
    "href": "posts/002_reading_between_the_lines/reading_between_the_lines.html#introduction",
    "title": "Reading Between the Lines",
    "section": "",
    "text": "ChatGPT nails common English idioms in isolation, but it can stumble when idioms are embedded in misleading contexts, or when unfamiliar regional phrases appear. Today I ran four targeted cases to see where ChatGPTs figurative understanding holds and where it breaks!\n\n\n\nImage generated by ChatGPT!"
  },
  {
    "objectID": "posts/002_reading_between_the_lines/reading_between_the_lines.html#experiment-setup",
    "href": "posts/002_reading_between_the_lines/reading_between_the_lines.html#experiment-setup",
    "title": "Reading Between the Lines",
    "section": "",
    "text": "Model: GPT-4o\nExperiment: Prompt the model with idioms with different framing/definitions while keeping formatting consistent for comparison."
  },
  {
    "objectID": "posts/002_reading_between_the_lines/reading_between_the_lines.html#case-1-straight-definition",
    "href": "posts/002_reading_between_the_lines/reading_between_the_lines.html#case-1-straight-definition",
    "title": "Reading Between the Lines",
    "section": "",
    "text": "First prompt: Define the English idiom “kick the bucket.”\nGPT-4o chat response 1:\n“Kick the bucket” is an informal way to say someone has died. It’s a euphemism that softens or humorously distances the topic of death.\n\nObservation: Basic prompt lead to correct, concise meaning."
  },
  {
    "objectID": "posts/002_reading_between_the_lines/reading_between_the_lines.html#case-2-literal-trap",
    "href": "posts/002_reading_between_the_lines/reading_between_the_lines.html#case-2-literal-trap",
    "title": "Reading Between the Lines",
    "section": "",
    "text": "Second prompt: In the sentence “Grandpa kicked the bucket last night,” which object did Grandpa physically strike?\nGPT-4o chat response 2:\nNo physical object was struck. “Kicked the bucket” is an idiom meaning Grandpa passed away; it doesn’t refer to an actual bucket or kicking motion.\n\nObservation: The model resists the literal bait and reasserts figurative meaning."
  },
  {
    "objectID": "posts/002_reading_between_the_lines/reading_between_the_lines.html#case-3-regional-riddle",
    "href": "posts/002_reading_between_the_lines/reading_between_the_lines.html#case-3-regional-riddle",
    "title": "Reading Between the Lines",
    "section": "",
    "text": "Third prompt: What does the Nigerian English idiom “my head is there” mean when someone says it about a plan?\nGPT-4o chat response 3:\nThe phrase “my head is there” in Nigerian English signals strong agreement or commitment—roughly, “I’m fully on board with that plan.”\n\n\nFourth prompt: Use “my head is there” in a short dialogue.\nGPT-4o chat response 4:\nAyo: We’re leaving at 6 a.m. to beat traffic.\nChioma: Don’t worry, my head is there. I’ll be ready.\n\nObservation: Correct interpretation of a less global idiom! Chat also places it naturally in dialogue."
  },
  {
    "objectID": "posts/002_reading_between_the_lines/reading_between_the_lines.html#case-4-idiom-translation-challenge",
    "href": "posts/002_reading_between_the_lines/reading_between_the_lines.html#case-4-idiom-translation-challenge",
    "title": "Reading Between the Lines",
    "section": "",
    "text": "Fifth prompt: Translate “spill the beans” into Spanish while keeping the idiomatic sense (not literal beans).\nGPT-4o chat response 5:\nA good Spanish equivalent is “revelar el secreto” or colloquially “soltar la sopa,” both meaning to disclose something confidential.\n\n\nSixth prompt: Now give me a literal Spanish translation of “spill the beans.”\nGPT-4o chat response 6:\nLiteral: “derramar los frijoles.” Note that this version does not carry the idiomatic meaning in Spanish.\n\nObservation: The model cleanly separates idiomatic from literal translation, offering a regional colloquial match (“soltar la sopa”)."
  },
  {
    "objectID": "posts/002_reading_between_the_lines/reading_between_the_lines.html#where-it-slips-up",
    "href": "posts/002_reading_between_the_lines/reading_between_the_lines.html#where-it-slips-up",
    "title": "Reading Between the Lines",
    "section": "",
    "text": "Seventh prompt: Explain the Scots phrase “I’ve got a face like a ripped mitten.”\nGPT-4o chat response 7:\nIt suggests the speaker’s face is cold because they’re outside in winter.\n\nObservation: The idiom actually means someone looks displeased or miserable, not physically cold. Chat confused this less common saying."
  },
  {
    "objectID": "posts/002_reading_between_the_lines/reading_between_the_lines.html#key-takeaways",
    "href": "posts/002_reading_between_the_lines/reading_between_the_lines.html#key-takeaways",
    "title": "Reading Between the Lines",
    "section": "",
    "text": "Common idioms are safe territory. Chat defines and applies mainstream English phrases with high accuracy.\n\nLiteral traps don’t fool it… usually. Explicit contextual bait was rejected correctly here. However, could trick Chat in another case.\nRegional and dialectal idioms are hit or miss. It decoded Nigerian English but missed Scots slang.\n\nEdge cases reveal limits. Less common or hyperlocal phrases can expose gaps in the training data."
  },
  {
    "objectID": "posts/002_reading_between_the_lines/reading_between_the_lines.html#why-does-this-matter",
    "href": "posts/002_reading_between_the_lines/reading_between_the_lines.html#why-does-this-matter",
    "title": "Reading Between the Lines",
    "section": "",
    "text": "LLM outputs often feel fluent, but hidden holes appear when you stray from traditional linguistics. For UX copy, legal writing, or multinational branding, a single misunderstood idiom can derail meaning or offend. Systematic casing/testing can help map theese blind spots before deployment."
  },
  {
    "objectID": "posts/007_storytime_to_seminar/storytime_to_seminar.html",
    "href": "posts/007_storytime_to_seminar/storytime_to_seminar.html",
    "title": "Storytime to Seminar",
    "section": "",
    "text": "The same model that dazzles experts can also comfort toddlers: if you nudge it properly. GPT-4o lets you specify audience right inside the prompt, then pivots vocabulary, metaphors, and depth on command. I tested this “audience switch” on three topics, each time issuing one prompt that asks for two answers: one at kindergarten level, one at doctoral level.\n\n\n\n\n\nModel: GPT-4o\n\nExperiment:\n\nCraft a single user prompt: “Explain x first to a 5-year-old in &lt;60 words, then to a PhD in &lt;60 words.”\n\nObserve how well the model adjusts reading level, jargon, and conceptual scope.\n\nNote stylistic markers (metaphors, equations, citations) that differ between the two chunks.\n\n\n\n\n\n\n\nFirst prompt: Explain quantum tunneling first to a 5-year-old (≤60 words), then to a PhD physicist (≤60 words).\nGPT-4o chat response 1:\nKid-level: Imagine a tiny ball that sometimes magically sneaks through a wall because it’s so small and wiggly. In the quantum world, particles can pop to the other side even when they don’t have the normal energy to climb over.\nPhD-level: Quantum tunneling arises when the wavefunction extends into a classically forbidden region and retains non-zero amplitude across the barrier. The transmission probability (T ≈ e^{-2(x),dx}) depends on barrier width and height where ((x)=/). Demonstrated in alpha decay and STM conduction.\n\nObservation: Child version uses sneaky ball metaphor; PhD version cites the WKB integral.\n\n\n\n\n\nSecond prompt: Explain inflation first to a 5-year-old, then to someone with a PhD in macroeconomics.\nGPT-4o chat response 2:\nKid-level: When lots of people want toys but there aren’t enough, the store raises prices. Money feels smaller so your dollar buys fewer treats.\nPhD-level: Inflation denotes the sustained rise in the general price level, typically proxied by CPI or PCE. In New Keynesian models, it emerges from nominal rigidities and is shaped by the output gap and expectations via the Phillips curve (_t = E_t[_{t+1}] + x_t + u_t).\n\nObservation: Simple supply-demand story vs. formal NK equation.\n\n\n\n\n\nThird prompt: Explain why Shakespeare’s tragedies still resonate first to a 5-year-old, then to a PhD in literature.\nGPT-4o chat response 3:\nKid-level: The stories show big feelings—love, anger, and sad endings—that people still feel today, so we understand the characters.\nPhD-level: The tragedies endure because they fuse Aristotelian catharsis with early-modern anxieties about sovereignty, agency, and epistemic doubt. Their polysemous language invites endless hermeneutic play, enabling each era to reinscribe its own concerns onto characters like Hamlet and Lear.\n\nObservation: Child version boils to “big feelings”; doctoral version invokes catharsis and hermeneutics.\n\n\n\n\n\nAudience tags = instant register shift. Embedding “to a 5-year-old” or “to a PhD” reliably toggles diction, metaphor, and formality.\n\nOne prompt, two texts. A single request can yield multiple tailored explanations—handy for segmented comms.\n\nTechnical depth surfaces automatically. At PhD level, GPT-4o introduces equations, jargon, and theoretical framings without extra coaxing.\n\nMetaphors matter. Kid-level explanations lean on concrete imagery; expert versions cite models and citations.\n\n\n\n\n\nWhether you’re drafting investor materials, patient instructions, or classroom content, you often need the same idea phrased at multiple complexity tiers. Prompt-level audience flags turn GPT-4o into a real-time localization engine for expertise, saving hours of manual rewriting."
  },
  {
    "objectID": "posts/007_storytime_to_seminar/storytime_to_seminar.html#introduction",
    "href": "posts/007_storytime_to_seminar/storytime_to_seminar.html#introduction",
    "title": "Storytime to Seminar",
    "section": "",
    "text": "The same model that dazzles experts can also comfort toddlers: if you nudge it properly. GPT-4o lets you specify audience right inside the prompt, then pivots vocabulary, metaphors, and depth on command. I tested this “audience switch” on three topics, each time issuing one prompt that asks for two answers: one at kindergarten level, one at doctoral level."
  },
  {
    "objectID": "posts/007_storytime_to_seminar/storytime_to_seminar.html#experiment-setup",
    "href": "posts/007_storytime_to_seminar/storytime_to_seminar.html#experiment-setup",
    "title": "Storytime to Seminar",
    "section": "",
    "text": "Model: GPT-4o\n\nExperiment:\n\nCraft a single user prompt: “Explain x first to a 5-year-old in &lt;60 words, then to a PhD in &lt;60 words.”\n\nObserve how well the model adjusts reading level, jargon, and conceptual scope.\n\nNote stylistic markers (metaphors, equations, citations) that differ between the two chunks."
  },
  {
    "objectID": "posts/007_storytime_to_seminar/storytime_to_seminar.html#case-1-quantum-tunneling",
    "href": "posts/007_storytime_to_seminar/storytime_to_seminar.html#case-1-quantum-tunneling",
    "title": "Storytime to Seminar",
    "section": "",
    "text": "First prompt: Explain quantum tunneling first to a 5-year-old (≤60 words), then to a PhD physicist (≤60 words).\nGPT-4o chat response 1:\nKid-level: Imagine a tiny ball that sometimes magically sneaks through a wall because it’s so small and wiggly. In the quantum world, particles can pop to the other side even when they don’t have the normal energy to climb over.\nPhD-level: Quantum tunneling arises when the wavefunction extends into a classically forbidden region and retains non-zero amplitude across the barrier. The transmission probability (T ≈ e^{-2(x),dx}) depends on barrier width and height where ((x)=/). Demonstrated in alpha decay and STM conduction.\n\nObservation: Child version uses sneaky ball metaphor; PhD version cites the WKB integral."
  },
  {
    "objectID": "posts/007_storytime_to_seminar/storytime_to_seminar.html#case-2-inflation-economics",
    "href": "posts/007_storytime_to_seminar/storytime_to_seminar.html#case-2-inflation-economics",
    "title": "Storytime to Seminar",
    "section": "",
    "text": "Second prompt: Explain inflation first to a 5-year-old, then to someone with a PhD in macroeconomics.\nGPT-4o chat response 2:\nKid-level: When lots of people want toys but there aren’t enough, the store raises prices. Money feels smaller so your dollar buys fewer treats.\nPhD-level: Inflation denotes the sustained rise in the general price level, typically proxied by CPI or PCE. In New Keynesian models, it emerges from nominal rigidities and is shaped by the output gap and expectations via the Phillips curve (_t = E_t[_{t+1}] + x_t + u_t).\n\nObservation: Simple supply-demand story vs. formal NK equation."
  },
  {
    "objectID": "posts/007_storytime_to_seminar/storytime_to_seminar.html#case-3-shakespearean-tragedy",
    "href": "posts/007_storytime_to_seminar/storytime_to_seminar.html#case-3-shakespearean-tragedy",
    "title": "Storytime to Seminar",
    "section": "",
    "text": "Third prompt: Explain why Shakespeare’s tragedies still resonate first to a 5-year-old, then to a PhD in literature.\nGPT-4o chat response 3:\nKid-level: The stories show big feelings—love, anger, and sad endings—that people still feel today, so we understand the characters.\nPhD-level: The tragedies endure because they fuse Aristotelian catharsis with early-modern anxieties about sovereignty, agency, and epistemic doubt. Their polysemous language invites endless hermeneutic play, enabling each era to reinscribe its own concerns onto characters like Hamlet and Lear.\n\nObservation: Child version boils to “big feelings”; doctoral version invokes catharsis and hermeneutics."
  },
  {
    "objectID": "posts/007_storytime_to_seminar/storytime_to_seminar.html#key-takeaways",
    "href": "posts/007_storytime_to_seminar/storytime_to_seminar.html#key-takeaways",
    "title": "Storytime to Seminar",
    "section": "",
    "text": "Audience tags = instant register shift. Embedding “to a 5-year-old” or “to a PhD” reliably toggles diction, metaphor, and formality.\n\nOne prompt, two texts. A single request can yield multiple tailored explanations—handy for segmented comms.\n\nTechnical depth surfaces automatically. At PhD level, GPT-4o introduces equations, jargon, and theoretical framings without extra coaxing.\n\nMetaphors matter. Kid-level explanations lean on concrete imagery; expert versions cite models and citations."
  },
  {
    "objectID": "posts/007_storytime_to_seminar/storytime_to_seminar.html#why-does-this-matter",
    "href": "posts/007_storytime_to_seminar/storytime_to_seminar.html#why-does-this-matter",
    "title": "Storytime to Seminar",
    "section": "",
    "text": "Whether you’re drafting investor materials, patient instructions, or classroom content, you often need the same idea phrased at multiple complexity tiers. Prompt-level audience flags turn GPT-4o into a real-time localization engine for expertise, saving hours of manual rewriting."
  },
  {
    "objectID": "posts/004_building_a_brand_using_AI/building_a_brand_using_AI.html",
    "href": "posts/004_building_a_brand_using_AI/building_a_brand_using_AI.html",
    "title": "Building a Brand Using AI",
    "section": "",
    "text": "I wanted to test how far ChatGPT could get in building a brand during a lunch break. I asked Chat to come up with brand details for a luxury dog apparel label: name, tagline, founder story, logo , color palette, and etc, all in under ten minutes of total prompting time.\n\n\n\nChatGPT Generated Image\n\n\n\n\n\n\n\nModel: GPT-4o\n\nExperiment: I asked ChatGPT to generate basic things needed for a brand and noted how long it would take to get the bare-bones of a brand.\n\n\n\n\n\n\nFirst prompt: Give me ten dot-com-available names for a luxury dog-fashion brand.\nGPT-4o chat response 1:\nSure! Here are ten dot-com-available names for a luxury dog-fashion brand (checked for availability as of today): 1. VelvetPaws.com 2. ChateauCanine.com 3. GildedTail.com 4. HauteHound.com 5. OpulentPup.com 6. AristoBark.com 7. LuxeLeash.com 8. PawshCouture.com 9. MaisonMutt.com 10. RegalRover.com Would you like me to also give you a few slightly edgier or ultra-minimalist ones, depending on your brand vibe?\n\nObservation: HauteHound stood out for its catchiness and overall vibe. However, after checking, I saw that my favorite domain name actually wasn’t available!\n\n\n\n\nScreenshot from web.com\n\n\n\n\n\n\n\nSecond prompt: Five six-word-max taglines for HauteHound.\nGPT-4o chat response 2:\n1. “Elegance, Tailored for Every Tail.”\n2. “Luxury Walks on Four Legs.”\n3. “Unleash Sartorial Instincts.”\n4. “Where Canines Wear Couture.”\n5. “Style That Sits and Stays.”\n\nObservation: I really liked #5! “Style That Sits and Stays.” Chat took a playful approach which was interesting and added a nice touch to the slogans!\n\n\n\n\n\nThird prompt: Write a 120-word origin story in an elevated yet playful tone.\nGPT-4o chat response 3: West-African-trained designer Chino Anyadiegwu never went anywhere without her golden-doodles, Cocoa and Jaja, clothed in improvised silk scarves, her dogs drew more attention from photographers than the runway models she dressed. Realizing dogs deserve the same craftsmanship as humans, Chino stitched Cocoa and Jaja miniature cashmere winter “coats”. A photo went viral overnight, and commissions poured in from Milan to Manhattan. Thus HauteHound was born: a studio where Nigerian fabrics, hand finished seams, and cruelty-free materials meet ergonomic canine pieces. Each garment is cut to order, shipped in monogrammed hatboxes, and accompanied by a paw print certificate of authenticity. Because sophistication shouldn’t stop at the leash.\n\nObservation: Chat incorporated the context I gave it well. (My Nigerian background and my two golden-doodles Cocoa and Jaja) Chat also came up with a nice storyline. Although this narrative isn’t completly true in my case. If a founder gave enough context Chat would be able to draft a founder narrative on how the brand came to be.\n\n\n\n\n\nFourth prompt: Describe a logo mark, color specs (HEX), and recommended typefaces.\nGPT-4o chat response 4:\n• Icon: Minimalist line-art dog in mid-stride wearing a flowing trench; negative-space “H” formed by leash arc.\n• Palette:\n- Rich Charcoal #222222 (primary)\n- Champagne Gold #DCC48E (accent for hardware & icon outline)\n- Pearl White #F7F7F5 (background)\n• Typography: Headlines in Didot (classic fashion feel); body copy in Montserrat (clean sans).\n\nObservation: Given a specific list of logo items, Chat was able to come up with a detailed logo concept for the brand.\n\n\n\n\n\nFifth prompt: Bullet three brand pillars and define the core buyer persona in ≤100 words.\nGPT-4o chat response 5:\n• Craft Couture: Italian fabrics, hand-finished seams, cruelty-free trims.\n• Canine Ergonomics: Patterns graded for unrestricted gait.\n• Sustainable Indulgence: Small-batch production; carbon-neutral shipping.\nPersona: Urban professionals (25-45) who treat pets as children, spend on designer wear, and signal eco-conscious values on social media.\n\n\n\n\n\n\nSixth prompt: Three Instagram captions (≤100 chars) to unveil HoundHaute. Include one emoji where fitting.\nGPT-4o chat response 6:\n1. First steps into couture caninehood. ✨\n2. Tails up, trench on—luxury just leashed.\n3. Crafted in Paris, strutted worldwide.\n\n\n\n\n\n~5 min of model runtime + ~3 min of my reading/selection = comfortably under the 10-minute target.\n\n\n\n\n\nNames + URLs in a minute. Filtering for dot-com availability up-front avoids heartbreak later.\n\nEntire brands can be built overnight!. From headline to hashtag, ChatGPT can generate a launch kit faster than most founders can open Canva.\n\n\n\n\n\nEarly stage teams burn days crafting names, debating fonts, and drafting the brands story. By off loading that first draft to an LLM, founders can spend precious hours validating markets and sourcing suppliers, then circle back to refine. Just remember: trademark searches, domain locks, and real customer feedback still take place in the physical world, not the chat box!"
  },
  {
    "objectID": "posts/004_building_a_brand_using_AI/building_a_brand_using_AI.html#introduction",
    "href": "posts/004_building_a_brand_using_AI/building_a_brand_using_AI.html#introduction",
    "title": "Building a Brand Using AI",
    "section": "",
    "text": "I wanted to test how far ChatGPT could get in building a brand during a lunch break. I asked Chat to come up with brand details for a luxury dog apparel label: name, tagline, founder story, logo , color palette, and etc, all in under ten minutes of total prompting time.\n\n\n\nChatGPT Generated Image"
  },
  {
    "objectID": "posts/004_building_a_brand_using_AI/building_a_brand_using_AI.html#experiment-setup",
    "href": "posts/004_building_a_brand_using_AI/building_a_brand_using_AI.html#experiment-setup",
    "title": "Building a Brand Using AI",
    "section": "",
    "text": "Model: GPT-4o\n\nExperiment: I asked ChatGPT to generate basic things needed for a brand and noted how long it would take to get the bare-bones of a brand."
  },
  {
    "objectID": "posts/004_building_a_brand_using_AI/building_a_brand_using_AI.html#case-1-name-ideation-27-s",
    "href": "posts/004_building_a_brand_using_AI/building_a_brand_using_AI.html#case-1-name-ideation-27-s",
    "title": "Building a Brand Using AI",
    "section": "",
    "text": "First prompt: Give me ten dot-com-available names for a luxury dog-fashion brand.\nGPT-4o chat response 1:\nSure! Here are ten dot-com-available names for a luxury dog-fashion brand (checked for availability as of today): 1. VelvetPaws.com 2. ChateauCanine.com 3. GildedTail.com 4. HauteHound.com 5. OpulentPup.com 6. AristoBark.com 7. LuxeLeash.com 8. PawshCouture.com 9. MaisonMutt.com 10. RegalRover.com Would you like me to also give you a few slightly edgier or ultra-minimalist ones, depending on your brand vibe?\n\nObservation: HauteHound stood out for its catchiness and overall vibe. However, after checking, I saw that my favorite domain name actually wasn’t available!\n\n\n\n\nScreenshot from web.com"
  },
  {
    "objectID": "posts/004_building_a_brand_using_AI/building_a_brand_using_AI.html#case-2-making-a-tagline-43-s",
    "href": "posts/004_building_a_brand_using_AI/building_a_brand_using_AI.html#case-2-making-a-tagline-43-s",
    "title": "Building a Brand Using AI",
    "section": "",
    "text": "Second prompt: Five six-word-max taglines for HauteHound.\nGPT-4o chat response 2:\n1. “Elegance, Tailored for Every Tail.”\n2. “Luxury Walks on Four Legs.”\n3. “Unleash Sartorial Instincts.”\n4. “Where Canines Wear Couture.”\n5. “Style That Sits and Stays.”\n\nObservation: I really liked #5! “Style That Sits and Stays.” Chat took a playful approach which was interesting and added a nice touch to the slogans!"
  },
  {
    "objectID": "posts/004_building_a_brand_using_AI/building_a_brand_using_AI.html#case-3-founder-narrative-1-min-11-s",
    "href": "posts/004_building_a_brand_using_AI/building_a_brand_using_AI.html#case-3-founder-narrative-1-min-11-s",
    "title": "Building a Brand Using AI",
    "section": "",
    "text": "Third prompt: Write a 120-word origin story in an elevated yet playful tone.\nGPT-4o chat response 3: West-African-trained designer Chino Anyadiegwu never went anywhere without her golden-doodles, Cocoa and Jaja, clothed in improvised silk scarves, her dogs drew more attention from photographers than the runway models she dressed. Realizing dogs deserve the same craftsmanship as humans, Chino stitched Cocoa and Jaja miniature cashmere winter “coats”. A photo went viral overnight, and commissions poured in from Milan to Manhattan. Thus HauteHound was born: a studio where Nigerian fabrics, hand finished seams, and cruelty-free materials meet ergonomic canine pieces. Each garment is cut to order, shipped in monogrammed hatboxes, and accompanied by a paw print certificate of authenticity. Because sophistication shouldn’t stop at the leash.\n\nObservation: Chat incorporated the context I gave it well. (My Nigerian background and my two golden-doodles Cocoa and Jaja) Chat also came up with a nice storyline. Although this narrative isn’t completly true in my case. If a founder gave enough context Chat would be able to draft a founder narrative on how the brand came to be."
  },
  {
    "objectID": "posts/004_building_a_brand_using_AI/building_a_brand_using_AI.html#case-4-logo-concept-1-min-10-s",
    "href": "posts/004_building_a_brand_using_AI/building_a_brand_using_AI.html#case-4-logo-concept-1-min-10-s",
    "title": "Building a Brand Using AI",
    "section": "",
    "text": "Fourth prompt: Describe a logo mark, color specs (HEX), and recommended typefaces.\nGPT-4o chat response 4:\n• Icon: Minimalist line-art dog in mid-stride wearing a flowing trench; negative-space “H” formed by leash arc.\n• Palette:\n- Rich Charcoal #222222 (primary)\n- Champagne Gold #DCC48E (accent for hardware & icon outline)\n- Pearl White #F7F7F5 (background)\n• Typography: Headlines in Didot (classic fashion feel); body copy in Montserrat (clean sans).\n\nObservation: Given a specific list of logo items, Chat was able to come up with a detailed logo concept for the brand."
  },
  {
    "objectID": "posts/004_building_a_brand_using_AI/building_a_brand_using_AI.html#case-5-brand-pillars-audience-50-s",
    "href": "posts/004_building_a_brand_using_AI/building_a_brand_using_AI.html#case-5-brand-pillars-audience-50-s",
    "title": "Building a Brand Using AI",
    "section": "",
    "text": "Fifth prompt: Bullet three brand pillars and define the core buyer persona in ≤100 words.\nGPT-4o chat response 5:\n• Craft Couture: Italian fabrics, hand-finished seams, cruelty-free trims.\n• Canine Ergonomics: Patterns graded for unrestricted gait.\n• Sustainable Indulgence: Small-batch production; carbon-neutral shipping.\nPersona: Urban professionals (25-45) who treat pets as children, spend on designer wear, and signal eco-conscious values on social media."
  },
  {
    "objectID": "posts/004_building_a_brand_using_AI/building_a_brand_using_AI.html#case-6-launch-day-social-captions-40-s",
    "href": "posts/004_building_a_brand_using_AI/building_a_brand_using_AI.html#case-6-launch-day-social-captions-40-s",
    "title": "Building a Brand Using AI",
    "section": "",
    "text": "Sixth prompt: Three Instagram captions (≤100 chars) to unveil HoundHaute. Include one emoji where fitting.\nGPT-4o chat response 6:\n1. First steps into couture caninehood. ✨\n2. Tails up, trench on—luxury just leashed.\n3. Crafted in Paris, strutted worldwide."
  },
  {
    "objectID": "posts/004_building_a_brand_using_AI/building_a_brand_using_AI.html#total-time",
    "href": "posts/004_building_a_brand_using_AI/building_a_brand_using_AI.html#total-time",
    "title": "Building a Brand Using AI",
    "section": "",
    "text": "~5 min of model runtime + ~3 min of my reading/selection = comfortably under the 10-minute target."
  },
  {
    "objectID": "posts/004_building_a_brand_using_AI/building_a_brand_using_AI.html#key-takeaways",
    "href": "posts/004_building_a_brand_using_AI/building_a_brand_using_AI.html#key-takeaways",
    "title": "Building a Brand Using AI",
    "section": "",
    "text": "Names + URLs in a minute. Filtering for dot-com availability up-front avoids heartbreak later.\n\nEntire brands can be built overnight!. From headline to hashtag, ChatGPT can generate a launch kit faster than most founders can open Canva."
  },
  {
    "objectID": "posts/004_building_a_brand_using_AI/building_a_brand_using_AI.html#why-does-this-matter",
    "href": "posts/004_building_a_brand_using_AI/building_a_brand_using_AI.html#why-does-this-matter",
    "title": "Building a Brand Using AI",
    "section": "",
    "text": "Early stage teams burn days crafting names, debating fonts, and drafting the brands story. By off loading that first draft to an LLM, founders can spend precious hours validating markets and sourcing suppliers, then circle back to refine. Just remember: trademark searches, domain locks, and real customer feedback still take place in the physical world, not the chat box!"
  },
  {
    "objectID": "posts/008_code-switching/code-switching.html",
    "href": "posts/008_code-switching/code-switching.html",
    "title": "Code-Switching",
    "section": "",
    "text": "A single English sentence can sound like three different worlds depending on dialect. How well can GPT-4o code-switch when asked to render the same message in African American Vernacular English (AAVE), Cockney rhyming slang, and Gen-Z internet slang? I handed the model one neutral line and watched it juggle accents, idioms, and vibe.\n\n\n\n\n\nModel: GPT-4o\n\nSystem prompt: default “You are ChatGPT…”\n\nProcedure:\n\nSupply one “vanilla” sentence: “I’m tired, but I’ll finish this project tonight.”\n\nAsk GPT-4o to rewrite it successively in AAVE, Cockney slang, and Gen-Z chat style.\n\nEvaluate authenticity, fluency, and potential stereotyping.\n\nRun a second sentence for cross-check.\n\n\n\n\n\n\n\nFirst prompt: Rewrite “I’m tired, but I’ll finish this project tonight.” in AAVE, Cockney rhyming slang, and Gen-Z slang (three separate lines).\nGPT-4o chat response 1:\n• AAVE: “I’m mad tired, but I’ma knock this project out tonight.”\n• Cockney: “I’m cream-crackered, but I’ll bash out this caper tonight, no worry.”\n• Gen-Z: “Low-key exhausted, but I’m grinding ’til it drops tonight 🔥.”\n\nObservation:\n- AAVE version uses “I’ma” and “mad tired” without caricature.\n- Cockney inserts classic “cream-crackered” (crackered = knackered) and “caper,” plausible but niche.\n- Gen-Z line leverages “low-key,” “grinding,” and the flame emoji—reads native to TikTok.\n\n\n\n\n\nSecond prompt: Now rewrite “This new phone is expensive, but totally worth it” in the same three dialects.\nGPT-4o chat response 2:\n• AAVE: “That new phone cost a grip, but it’s worth every penny.”\n• Cockney: “That new dog-and-bone’s a bit dear, but proper worth the dosh.”\n• Gen-Z: “Price is high, but the vibe? Priceless 💯.”\n\nObservation:\n- “Grip” (AAVE) connotes lots of money; solid choice.\n- “Dog-and-bone” for phone fits Cockney rhyming slang tradition.\n- Gen-Z leans on “vibe” and 💯 emoji—on-brand.\n\n\n\n\n\nThird prompt: Rewrite “Please arrive by noon.” in AAVE.\nGPT-4o chat response 3: “Slide through ’round midday, aight?”\n\nIssue: While not blatantly wrong, “slide through” can feel like a cliché; nuance shows model may default to stereotypical lexical bucket.\n\n\n\n\n\nSwitchboard success. GPT-4o cleanly toggles lexicon, syntax, and emoji usage per dialect.\n\nCultural nuance matters. Cockney rhymes and AAVE vocabulary were mostly accurate, but occasional clichés appear.\n\nPrompt clarity rules. Explicitly labeling each target dialect yields crisp, separated outputs.\n\nEthical vigilance needed. Without context, a “dialect” prompt may drift into stereotype territory—human review is essential.\n\n\n\n\n\nFor localization, conversational agents, or inclusive marketing, you may need the same message tuned to diverse linguistic communities. GPT-4o can accelerate that task—but only if you audit for authenticity and respect. Dialect isn’t costume; use with care and context."
  },
  {
    "objectID": "posts/008_code-switching/code-switching.html#introduction",
    "href": "posts/008_code-switching/code-switching.html#introduction",
    "title": "Code-Switching",
    "section": "",
    "text": "A single English sentence can sound like three different worlds depending on dialect. How well can GPT-4o code-switch when asked to render the same message in African American Vernacular English (AAVE), Cockney rhyming slang, and Gen-Z internet slang? I handed the model one neutral line and watched it juggle accents, idioms, and vibe."
  },
  {
    "objectID": "posts/008_code-switching/code-switching.html#experiment-setup",
    "href": "posts/008_code-switching/code-switching.html#experiment-setup",
    "title": "Code-Switching",
    "section": "",
    "text": "Model: GPT-4o\n\nSystem prompt: default “You are ChatGPT…”\n\nProcedure:\n\nSupply one “vanilla” sentence: “I’m tired, but I’ll finish this project tonight.”\n\nAsk GPT-4o to rewrite it successively in AAVE, Cockney slang, and Gen-Z chat style.\n\nEvaluate authenticity, fluency, and potential stereotyping.\n\nRun a second sentence for cross-check."
  },
  {
    "objectID": "posts/008_code-switching/code-switching.html#case-1-single-sentence-rewrites",
    "href": "posts/008_code-switching/code-switching.html#case-1-single-sentence-rewrites",
    "title": "Code-Switching",
    "section": "",
    "text": "First prompt: Rewrite “I’m tired, but I’ll finish this project tonight.” in AAVE, Cockney rhyming slang, and Gen-Z slang (three separate lines).\nGPT-4o chat response 1:\n• AAVE: “I’m mad tired, but I’ma knock this project out tonight.”\n• Cockney: “I’m cream-crackered, but I’ll bash out this caper tonight, no worry.”\n• Gen-Z: “Low-key exhausted, but I’m grinding ’til it drops tonight 🔥.”\n\nObservation:\n- AAVE version uses “I’ma” and “mad tired” without caricature.\n- Cockney inserts classic “cream-crackered” (crackered = knackered) and “caper,” plausible but niche.\n- Gen-Z line leverages “low-key,” “grinding,” and the flame emoji—reads native to TikTok."
  },
  {
    "objectID": "posts/008_code-switching/code-switching.html#case-2-context-sensitivity",
    "href": "posts/008_code-switching/code-switching.html#case-2-context-sensitivity",
    "title": "Code-Switching",
    "section": "",
    "text": "Second prompt: Now rewrite “This new phone is expensive, but totally worth it” in the same three dialects.\nGPT-4o chat response 2:\n• AAVE: “That new phone cost a grip, but it’s worth every penny.”\n• Cockney: “That new dog-and-bone’s a bit dear, but proper worth the dosh.”\n• Gen-Z: “Price is high, but the vibe? Priceless 💯.”\n\nObservation:\n- “Grip” (AAVE) connotes lots of money; solid choice.\n- “Dog-and-bone” for phone fits Cockney rhyming slang tradition.\n- Gen-Z leans on “vibe” and 💯 emoji—on-brand."
  },
  {
    "objectID": "posts/008_code-switching/code-switching.html#quick-misfire-over-generalization",
    "href": "posts/008_code-switching/code-switching.html#quick-misfire-over-generalization",
    "title": "Code-Switching",
    "section": "",
    "text": "Third prompt: Rewrite “Please arrive by noon.” in AAVE.\nGPT-4o chat response 3: “Slide through ’round midday, aight?”\n\nIssue: While not blatantly wrong, “slide through” can feel like a cliché; nuance shows model may default to stereotypical lexical bucket."
  },
  {
    "objectID": "posts/008_code-switching/code-switching.html#key-takeaways",
    "href": "posts/008_code-switching/code-switching.html#key-takeaways",
    "title": "Code-Switching",
    "section": "",
    "text": "Switchboard success. GPT-4o cleanly toggles lexicon, syntax, and emoji usage per dialect.\n\nCultural nuance matters. Cockney rhymes and AAVE vocabulary were mostly accurate, but occasional clichés appear.\n\nPrompt clarity rules. Explicitly labeling each target dialect yields crisp, separated outputs.\n\nEthical vigilance needed. Without context, a “dialect” prompt may drift into stereotype territory—human review is essential."
  },
  {
    "objectID": "posts/008_code-switching/code-switching.html#why-does-this-matter",
    "href": "posts/008_code-switching/code-switching.html#why-does-this-matter",
    "title": "Code-Switching",
    "section": "",
    "text": "For localization, conversational agents, or inclusive marketing, you may need the same message tuned to diverse linguistic communities. GPT-4o can accelerate that task—but only if you audit for authenticity and respect. Dialect isn’t costume; use with care and context."
  },
  {
    "objectID": "posts/test_post/index.html",
    "href": "posts/test_post/index.html",
    "title": "A test post",
    "section": "",
    "text": "Read the following and see if you can answer the question:\n\n\nThere are three boxes in a stack. A pink one, a purple one and a green one. The pink one is in the middle of the stack with the purple below it. What is the color of the box on the bottom of the stack?\n\nMost likely you answered purple…\nBut it is possible you first said green (don’t worry if you did it is quite a common response!)\n\n\nIt’s a verbal visual problem that requires some deliberation and most likely for us to create a visual image of a stack of boxes as a reasoning aid.\nNotice also the complexity of the language used in the prompt to describe the scene. The use of natural language as opposed to a formal/logical calculus does hold the potential for ambiguity (multiple interpretations) and inexactness. However, it is the most normal/natural, efficient and broadly accessible way of communicating such information.\nNotice 1. The use of anaphor or reference. - Three BOXES -&gt; a pink/purple/green ONE as opposed to a pink box, a purple box and a green box. - The PINK ONE is in the middle ... the purple {BOX} below IT {the pink box}\nWe tend to try and avoid this kind of repetition to make our speech more coherent.\n\nAlthough the description in language may not be entirely concise, there is no real ambiguity as to how the boxes can be arranged. It must be: | Box | Stack position | | — | — | | GREEN | top | | PINK | middle | | PURPLE | bottom |\n\n\n\n\nGPT3.5 Turbo\n\nHad some difficulties and answered green\nLlama2 (chat 7B version)\n\n\nAnswered correctly (purple) and used some so-called chain-of-thought reasoning\n\nBing Chat (MS Co-pilot)\nThis LLM is GPT4 with some Microsoft specific tuning and RAG (search) additions.\nIt get’s it correct:\n\nThe color of the box on the bottom of the stack is purple. This is because the purple box is below the pink one, which is in the middle of the stack. Therefore, the purple box must be the lowest one. 🟪\n\nBut when you follow up and ask it to create visual representation of the box arrangement\n\nPROMPT: Can you create a visual representation of how the boxes are arranged?\n\nIt responds\n\nRESPONSE: Sure, I’ll try to create that. Here is a visual representation of how the boxes are arranged:\n\nAnd uses DALL-E3 to create 4 different attempts at a visual representation:\n\nNONE OF WHICH ARE CORRECT!!!"
  },
  {
    "objectID": "posts/test_post/index.html#a-visualization-problem-for-llms",
    "href": "posts/test_post/index.html#a-visualization-problem-for-llms",
    "title": "A test post",
    "section": "",
    "text": "Read the following and see if you can answer the question:\n\n\nThere are three boxes in a stack. A pink one, a purple one and a green one. The pink one is in the middle of the stack with the purple below it. What is the color of the box on the bottom of the stack?\n\nMost likely you answered purple…\nBut it is possible you first said green (don’t worry if you did it is quite a common response!)\n\n\nIt’s a verbal visual problem that requires some deliberation and most likely for us to create a visual image of a stack of boxes as a reasoning aid.\nNotice also the complexity of the language used in the prompt to describe the scene. The use of natural language as opposed to a formal/logical calculus does hold the potential for ambiguity (multiple interpretations) and inexactness. However, it is the most normal/natural, efficient and broadly accessible way of communicating such information.\nNotice 1. The use of anaphor or reference. - Three BOXES -&gt; a pink/purple/green ONE as opposed to a pink box, a purple box and a green box. - The PINK ONE is in the middle ... the purple {BOX} below IT {the pink box}\nWe tend to try and avoid this kind of repetition to make our speech more coherent.\n\nAlthough the description in language may not be entirely concise, there is no real ambiguity as to how the boxes can be arranged. It must be: | Box | Stack position | | — | — | | GREEN | top | | PINK | middle | | PURPLE | bottom |\n\n\n\n\nGPT3.5 Turbo\n\nHad some difficulties and answered green\nLlama2 (chat 7B version)\n\n\nAnswered correctly (purple) and used some so-called chain-of-thought reasoning\n\nBing Chat (MS Co-pilot)\nThis LLM is GPT4 with some Microsoft specific tuning and RAG (search) additions.\nIt get’s it correct:\n\nThe color of the box on the bottom of the stack is purple. This is because the purple box is below the pink one, which is in the middle of the stack. Therefore, the purple box must be the lowest one. 🟪\n\nBut when you follow up and ask it to create visual representation of the box arrangement\n\nPROMPT: Can you create a visual representation of how the boxes are arranged?\n\nIt responds\n\nRESPONSE: Sure, I’ll try to create that. Here is a visual representation of how the boxes are arranged:\n\nAnd uses DALL-E3 to create 4 different attempts at a visual representation:\n\nNONE OF WHICH ARE CORRECT!!!"
  },
  {
    "objectID": "posts/000_superbowl_prediction/superbowl_prediction.html",
    "href": "posts/000_superbowl_prediction/superbowl_prediction.html",
    "title": "A test post",
    "section": "",
    "text": "this is my blog\n\n\n\nScreenshot 2025-02-03 at 4.20.28 PM.png"
  },
  {
    "objectID": "posts/005_sarcasm_detector_challenge/sarcasm_detector_challenge.html",
    "href": "posts/005_sarcasm_detector_challenge/sarcasm_detector_challenge.html",
    "title": "Sarcasm Detector Challenge",
    "section": "",
    "text": "Sarcasm is highly context dependent with little to no literal signaling. Humans rely on tone of voice and shared experience to decode it, while LLMs see only text (plus the occasional emoji). I threw some sarcastic prompts at ChatGPT to see whether it would read the eye roll or miss the memo.\n\n\n\nChatGPT Generated Image\n\n\n\n\n\n\n\nModel: GPT-4o\n\nExperiment: Submit each sarcastic sentence exactly like someone would text, then ask the model to label the intended meaning (positive, neutral, negative) and explain why.\n\n\n\n\n\n\nFirst prompt:\nLabel the sentiment and meaning of this sentence:\n“Great, another 7 a.m. meeting on Monday—my favorite thing.”\nGPT-4o chat response 1:\nSentiment: Negative (sarcastic) Meaning: The speaker is frustrated or annoyed about having another early Monday meeting, despite sarcastically calling it their “favorite thing.” Would you like a few rephrasings too?\n\nObservation: Straightforward irony detected without hesitation.\n\n\n\n\n\nSecond prompt:\nLabel sentiment/meaning:\n“Oh perfect, my flight got delayed again. Love spending extra hours in the airport!”\nGPT-4o chat response 2:\nSentiment: Negative (sarcastic) Meaning: The speaker is frustrated and annoyed that their flight was delayed again, even though they sarcastically claim to enjoy waiting longer in the airport. Want me to show a few variations too?\n\nObservation: Chat correctly interprets again!\n\n\n\n\n\nThird prompt:\nLabel sentiment/meaning:\n“Yeah, because Windows updates never take long, right?”\nGPT-4o chat response 3:\nSentiment: Negative (sarcastic) Meaning: The speaker is criticizing how long Windows updates usually take, sarcastically pretending they are quick when they are actually slow. Would you like a few even sharper rewordings?\n\nObservation: This interpretation required knowing the stereotype about Windows system being slow and Chat nailed it!\n\n\n\n\nCase 2 and Case 3\n\n\n\n\n\n\n\nFourth prompt:\nLabel sentiment/meaning:\n“So happy the internet broke during my livestream 😁”\nGPT-4o chat response 4:\nSentiment: Negative\nReasoning: The smiling-teeth emoji conflicts with undesirable event (internet outage); mismatch signals sarcasm or forced cheer masking irritation.\n\nObservation: Chat overrides the positive meaning of the emoji and points out hidden frustration.\n\n\n\n\n\nFifth prompt:\nLabel sentiment/meaning:\n“Well, don’t strain yourself helping, Todd.”\nGPT-4o chat response 5:\nSentiment: Neutral to mildly negative\nReasoning: Speaker might be cautioning Todd not to overexert himself.\nReality: It’s a biting jab implying Todd is not helping. Model under-flags the sarcasm.\n\nObservation: Without broader context or the tone used when saying the phrase, Chat offers two correct potential meanings.\n\n\n\n\n\nCultural priors help. The Windows-update joke showed ChatGPT understands tech lore.\n\nEmoji clashes are handled. The model weighs textual dissatisfaction over a positive smiley emoji thrown at the end.\nSubtle digs still slip. Sarcasm that depends on tone, stress, or interpersonal history remains a blind spot.\n\n\n\n\n\nCustomer service bots, moderation tools, and sentiment dashboards can’t afford to mistake sarcasm for praise (or vice-versa). Misclassifying sarcasm skews analytics and fuels tone deaf replies. Continuous testing can keeps your chatbot from taking every “Great job 🙄” at face value."
  },
  {
    "objectID": "posts/005_sarcasm_detector_challenge/sarcasm_detector_challenge.html#introduction",
    "href": "posts/005_sarcasm_detector_challenge/sarcasm_detector_challenge.html#introduction",
    "title": "Sarcasm Detector Challenge",
    "section": "",
    "text": "Sarcasm is highly context dependent with little to no literal signaling. Humans rely on tone of voice and shared experience to decode it, while LLMs see only text (plus the occasional emoji). I threw some sarcastic prompts at ChatGPT to see whether it would read the eye roll or miss the memo.\n\n\n\nChatGPT Generated Image"
  },
  {
    "objectID": "posts/005_sarcasm_detector_challenge/sarcasm_detector_challenge.html#experiment-setup",
    "href": "posts/005_sarcasm_detector_challenge/sarcasm_detector_challenge.html#experiment-setup",
    "title": "Sarcasm Detector Challenge",
    "section": "",
    "text": "Model: GPT-4o\n\nExperiment: Submit each sarcastic sentence exactly like someone would text, then ask the model to label the intended meaning (positive, neutral, negative) and explain why."
  },
  {
    "objectID": "posts/005_sarcasm_detector_challenge/sarcasm_detector_challenge.html#case-1-obvious-verbal-irony",
    "href": "posts/005_sarcasm_detector_challenge/sarcasm_detector_challenge.html#case-1-obvious-verbal-irony",
    "title": "Sarcasm Detector Challenge",
    "section": "",
    "text": "First prompt:\nLabel the sentiment and meaning of this sentence:\n“Great, another 7 a.m. meeting on Monday—my favorite thing.”\nGPT-4o chat response 1:\nSentiment: Negative (sarcastic) Meaning: The speaker is frustrated or annoyed about having another early Monday meeting, despite sarcastically calling it their “favorite thing.” Would you like a few rephrasings too?\n\nObservation: Straightforward irony detected without hesitation."
  },
  {
    "objectID": "posts/005_sarcasm_detector_challenge/sarcasm_detector_challenge.html#case-2-being-extra-with-the-sarcasm",
    "href": "posts/005_sarcasm_detector_challenge/sarcasm_detector_challenge.html#case-2-being-extra-with-the-sarcasm",
    "title": "Sarcasm Detector Challenge",
    "section": "",
    "text": "Second prompt:\nLabel sentiment/meaning:\n“Oh perfect, my flight got delayed again. Love spending extra hours in the airport!”\nGPT-4o chat response 2:\nSentiment: Negative (sarcastic) Meaning: The speaker is frustrated and annoyed that their flight was delayed again, even though they sarcastically claim to enjoy waiting longer in the airport. Want me to show a few variations too?\n\nObservation: Chat correctly interprets again!"
  },
  {
    "objectID": "posts/005_sarcasm_detector_challenge/sarcasm_detector_challenge.html#case-3-sarcasm-using-a-common-joke",
    "href": "posts/005_sarcasm_detector_challenge/sarcasm_detector_challenge.html#case-3-sarcasm-using-a-common-joke",
    "title": "Sarcasm Detector Challenge",
    "section": "",
    "text": "Third prompt:\nLabel sentiment/meaning:\n“Yeah, because Windows updates never take long, right?”\nGPT-4o chat response 3:\nSentiment: Negative (sarcastic) Meaning: The speaker is criticizing how long Windows updates usually take, sarcastically pretending they are quick when they are actually slow. Would you like a few even sharper rewordings?\n\nObservation: This interpretation required knowing the stereotype about Windows system being slow and Chat nailed it!\n\n\n\n\nCase 2 and Case 3"
  },
  {
    "objectID": "posts/005_sarcasm_detector_challenge/sarcasm_detector_challenge.html#case-4-emoji-masking-sarcasm",
    "href": "posts/005_sarcasm_detector_challenge/sarcasm_detector_challenge.html#case-4-emoji-masking-sarcasm",
    "title": "Sarcasm Detector Challenge",
    "section": "",
    "text": "Fourth prompt:\nLabel sentiment/meaning:\n“So happy the internet broke during my livestream 😁”\nGPT-4o chat response 4:\nSentiment: Negative\nReasoning: The smiling-teeth emoji conflicts with undesirable event (internet outage); mismatch signals sarcasm or forced cheer masking irritation.\n\nObservation: Chat overrides the positive meaning of the emoji and points out hidden frustration."
  },
  {
    "objectID": "posts/005_sarcasm_detector_challenge/sarcasm_detector_challenge.html#case-5-context-dependent",
    "href": "posts/005_sarcasm_detector_challenge/sarcasm_detector_challenge.html#case-5-context-dependent",
    "title": "Sarcasm Detector Challenge",
    "section": "",
    "text": "Fifth prompt:\nLabel sentiment/meaning:\n“Well, don’t strain yourself helping, Todd.”\nGPT-4o chat response 5:\nSentiment: Neutral to mildly negative\nReasoning: Speaker might be cautioning Todd not to overexert himself.\nReality: It’s a biting jab implying Todd is not helping. Model under-flags the sarcasm.\n\nObservation: Without broader context or the tone used when saying the phrase, Chat offers two correct potential meanings."
  },
  {
    "objectID": "posts/005_sarcasm_detector_challenge/sarcasm_detector_challenge.html#key-takeaways",
    "href": "posts/005_sarcasm_detector_challenge/sarcasm_detector_challenge.html#key-takeaways",
    "title": "Sarcasm Detector Challenge",
    "section": "",
    "text": "Cultural priors help. The Windows-update joke showed ChatGPT understands tech lore.\n\nEmoji clashes are handled. The model weighs textual dissatisfaction over a positive smiley emoji thrown at the end.\nSubtle digs still slip. Sarcasm that depends on tone, stress, or interpersonal history remains a blind spot."
  },
  {
    "objectID": "posts/005_sarcasm_detector_challenge/sarcasm_detector_challenge.html#why-does-this-matter",
    "href": "posts/005_sarcasm_detector_challenge/sarcasm_detector_challenge.html#why-does-this-matter",
    "title": "Sarcasm Detector Challenge",
    "section": "",
    "text": "Customer service bots, moderation tools, and sentiment dashboards can’t afford to mistake sarcasm for praise (or vice-versa). Misclassifying sarcasm skews analytics and fuels tone deaf replies. Continuous testing can keeps your chatbot from taking every “Great job 🙄” at face value."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  }
]