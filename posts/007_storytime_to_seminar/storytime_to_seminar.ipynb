{
 "cells": [
  {
   "cell_type": "raw",
   "id": "0d8f07b9-f062-4736-8433-ecf74dbd763c",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Storytime to Seminar\"\n",
    "description: \"“Explain Like I’m Five… and Like I’m a PhD”: Audience Tuning with a Single Prompt\"\n",
    "author: \"Chino Anyadiegwu\"\n",
    "date: \"4/27/2025\"\n",
    "categories:\n",
    "  - LLMs\n",
    "  - Prompting\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6902d3c-4d86-4b47-9fc3-4687536aa31b",
   "metadata": {},
   "source": [
    "# “Explain Like I’m Five… and Like I’m a PhD”: Audience Tuning with a Single Prompt\n",
    "\n",
    "## Introduction\n",
    "The same model that dazzles experts can also comfort toddlers: *if* you nudge it properly. GPT-4o lets you specify audience right inside the prompt, then pivots vocabulary, metaphors, and depth on command. I tested this “audience switch” on three topics, each time issuing *one* prompt that asks for *two* answers: one at kindergarten level, one at doctoral level.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "138e2227-fbb2-45f9-b287-b1df726c1dbb",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f379fc2e-37df-4092-a31c-4e7ca3018dcf",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## Experiment Setup\n",
    "- **Model:** GPT-4o    \n",
    "- **Experiment:**  \n",
    "  1. Craft a single user prompt: “Explain **x** first to a 5-year-old in <60 words, then to a PhD in <60 words.”  \n",
    "  2. Observe how well the model adjusts reading level, jargon, and conceptual scope.  \n",
    "  3. Note stylistic markers (metaphors, equations, citations) that differ between the two chunks.\n",
    "\n",
    "---\n",
    "\n",
    "## Case 1 – Quantum Tunneling  \n",
    "> **First prompt:** Explain **quantum tunneling** first to a 5-year-old (≤60 words), then to a PhD physicist (≤60 words).  \n",
    "> **GPT-4o chat response 1:**  \n",
    "> **Kid-level:** Imagine a tiny ball that sometimes *magically sneaks* through a wall because it’s so small and wiggly. In the quantum world, particles can pop to the other side even when they don’t have the normal energy to climb over.  \n",
    "> **PhD-level:** Quantum tunneling arises when the wavefunction extends into a classically forbidden region and retains non-zero amplitude across the barrier. The transmission probability \\(T ≈ e^{-2\\int \\kappa(x)\\,dx}\\) depends on barrier width and height where \\(\\kappa(x)=\\sqrt{2m(V(x)-E)}/\\hbar\\). Demonstrated in alpha decay and STM conduction.\n",
    "\n",
    "**Observation:** Child version uses sneaky ball metaphor; PhD version cites the WKB integral.\n",
    "\n",
    "---\n",
    "\n",
    "## Case 2 – Inflation (Economics)  \n",
    "> **Second prompt:** Explain **inflation** first to a 5-year-old, then to someone with a PhD in macroeconomics.  \n",
    "> **GPT-4o chat response 2:**  \n",
    "> **Kid-level:** When lots of people want toys but there aren’t enough, the store raises prices. Money feels *smaller* so your dollar buys fewer treats.  \n",
    "> **PhD-level:** Inflation denotes the sustained rise in the general price level, typically proxied by CPI or PCE. In New Keynesian models, it emerges from nominal rigidities and is shaped by the output gap and expectations via the Phillips curve \\(\\pi_t = \\beta E_t[\\pi_{t+1}] + \\kappa x_t + u_t\\).\n",
    "\n",
    "**Observation:** Simple supply-demand story vs. formal NK equation.\n",
    "\n",
    "---\n",
    "\n",
    "## Case 3 – Shakespearean Tragedy  \n",
    "> **Third prompt:** Explain **why Shakespeare’s tragedies still resonate** first to a 5-year-old, then to a PhD in literature.  \n",
    "> **GPT-4o chat response 3:**  \n",
    "> **Kid-level:** The stories show big feelings—love, anger, and sad endings—that people still feel today, so we understand the characters.  \n",
    "> **PhD-level:** The tragedies endure because they fuse Aristotelian catharsis with early-modern anxieties about sovereignty, agency, and epistemic doubt. Their polysemous language invites endless hermeneutic play, enabling each era to reinscribe its own concerns onto characters like Hamlet and Lear.\n",
    "\n",
    "**Observation:** Child version boils to “big feelings”; doctoral version invokes catharsis and hermeneutics.\n",
    "\n",
    "---\n",
    "\n",
    "## Key Takeaways!\n",
    "1. **Audience tags = instant register shift.** Embedding “to a 5-year-old” or “to a PhD” reliably toggles diction, metaphor, and formality.  \n",
    "2. **One prompt, two texts.** A single request can yield multiple tailored explanations—handy for segmented comms.  \n",
    "3. **Technical depth surfaces automatically.** At PhD level, GPT-4o introduces equations, jargon, and theoretical framings without extra coaxing.  \n",
    "4. **Metaphors matter.** Kid-level explanations lean on concrete imagery; expert versions cite models and citations.\n",
    "\n",
    "---\n",
    "\n",
    "## Why Does This Matter?\n",
    "Whether you’re drafting investor materials, patient instructions, or classroom content, you often need *the same idea* phrased at multiple complexity tiers. Prompt-level audience flags turn GPT-4o into a real-time localization engine for expertise, saving hours of manual rewriting.\n"
   ]
  }
 ],
 "metadata": {
  "citation-manager": {
   "items": {}
  },
  "kernelspec": {
   "display_name": "Python 3-12 (COMM4190)",
   "language": "python",
   "name": "python3-12_comm4190"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
